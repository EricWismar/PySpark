{"cells":[{"cell_type":"code","source":["%run ./00_functions_and_libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import required libraries and define functions","showTitle":true,"inputWidgets":{},"nuid":"07be8025-9fe7-4ef2-a525-04b1efe0a6fc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run ./01_params"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Creates dict containing required variables, filepaths etc.","showTitle":true,"inputWidgets":{},"nuid":"b746b74b-64e6-429a-82e4-7e661c94f37d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: &#39;if params[\\&#39;Database\\&#39;] == &#34;prod&#34;:\\n  params[\\&#39;RF_LOC\\&#39;] = &#34;X&#34;\\nelif params[\\&#39;Database\\&#39;] == &#34;impl&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;Database\\&#39;]==&#34;val&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;db\\&#39;]==&#34;dev1&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;Database\\&#39;] == &#34;dev0&#34;:\\n  params[\\&#39;RF_LOC\\&#39;]\\nelse:\\n  raise ValueError(\\&#39;{} is invalid or unmapped\\&#39;.format(db))&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: &#39;if params[\\&#39;Database\\&#39;] == &#34;prod&#34;:\\n  params[\\&#39;RF_LOC\\&#39;] = &#34;X&#34;\\nelif params[\\&#39;Database\\&#39;] == &#34;impl&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;Database\\&#39;]==&#34;val&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;db\\&#39;]==&#34;dev1&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;Database\\&#39;] == &#34;dev0&#34;:\\n  params[\\&#39;RF_LOC\\&#39;]\\nelse:\\n  raise ValueError(\\&#39;{} is invalid or unmapped\\&#39;.format(db))&#39;</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set('spark.sql.execution.arrow.enabled', True)\nspark.conf.set('spark.sql.execution.arrow.fallback.enabled', False)\nspark.conf.set(\"spark.databricks.io.cache.enabled\", True)\nspark.conf.set(\"spark.sql.session.timeZone\", \"America/New_York\")\nspark.conf.set(\n  params[\"AzureSASLocation\"],\n  dbutils.secrets.get(scope=params[\"AzureSASScope\"],key=params[\"AzureSASKey\"])\n)\nspark.sql('use {}'.format(params[\"Database\"]))\nintDataDir=params['intermediateDataDir']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Environment Config","showTitle":true,"inputWidgets":{},"nuid":"f7bac72b-6b19-4ac8-8903-59bc3ed358c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["''' \n  The below loop is to replicate MySQL group concat functionality that is done via sort_array and slice. \n  For a list of columns that need to be group concated through, evaluate each for a given record (group by column, MP_ID in this case),\n  aggregating all of the vlues for a given column from concat list, put into a list column (via collect_list) and use sort_array to \n  sort them within the column based on the respective PCR's PM Score (in descending order). Then use concat_ws to take the first item\n  from that sorted list column.\n  \n  df = the dataframe of interest\n  group_column = the value to group by, i.e. the id column the dataframe is being rolled up to\n  concat_list = a list of strings giving column names wanting to apply the group concat\n  sort_column = a string of the column name to be used to sort by\n  sort_order = False ; by default will sort in desc\n'''  \n\ngroup_column = \"mp_id\"\nconcat_list = ['last_name', 'first_name', 'middle_name', 'address', 'city', 'county', 'state', 'zip_code', 'ssn', 'gender', 'race', 'age', 'dob', 'state_issuing_drivers_license', 'drivers_license_number', 'mbi']\nsort_column = \"pm_score\"\nsort_order = False\ndf=spark.sql('select * from pcr_master')\n\ndf=df.fillna(100.00,subset=[\"pm_score\"])\ndf_final=df.select(group_column).distinct()\ndf_final=df_final.withColumn('er_flag',lit(None))\nfor i in concat_list:\n  df_helper=df.groupBy(group_column)\\\n  .agg(sort_array(collect_list(struct(sort_column,i)),sort_order).alias('collect_list'))\\\n  .withColumn(\"sorted_list\",col(\"collect_list.\"+str(i)))\\\n  .withColumn(\"first_item\",slice(col(\"sorted_list\"),1,1))\\\n  .withColumn(i,concat_ws(\",\",col(\"first_item\")))\\\n  .drop(\"collect_list\")\\\n  .drop(\"sorted_list\")\\\n  .drop(\"first_item\")\n  df_final=df_final.join(df_helper,group_column,\"inner\")\n  df_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{intDataDir}/delta/GOLD/pcr_meta\")\n  df_final = spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/pcr_meta\")\ndf_final = convertToNull(df_final)  \ndf_final = df_final.withColumn('er_flag', when(nonnull_count(col('last_name'), col('first_name'), col('middle_name'), col('address'), col('city'), col('county'), col('state'), col('zip_code'), col('ssn'), col('gender'), col('race'), col('age'), col('dob'), col('state_issuing_drivers_license'), col('drivers_license_number'), col('mbi')) > 10, lit(1)).otherwise(lit(0)))\ndf_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{intDataDir}/delta/GOLD/pcr_meta\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7f90220-eb6a-479d-885e-678388bb5e43"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"12_Create_PCR_Meta_Data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2261646131270747}},"nbformat":4,"nbformat_minor":0}
