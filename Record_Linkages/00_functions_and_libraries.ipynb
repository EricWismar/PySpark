{"cells":[{"cell_type":"code","source":["spark.conf.set('spark.sql.execution.arrow.enabled', True)\nspark.conf.set('spark.sql.execution.arrow.fallback.enabled', False)\nimport pandas as pd\nimport string\nimport os\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import array, array_distinct, broadcast, coalesce, col, column,collect_set, count, avg,\\\n                                  concat, concat_ws, create_map, explode, first, lit, lower, upper,lag,\\\n                                  monotonically_increasing_id, pandas_udf, PandasUDFType,stddev_pop,\\\n                                  regexp_extract, sha2, size, split, trim, udf, when, round,months_between,\\\n                                  substring, current_date, datediff, year, current_timestamp, udf, monotonically_increasing_id,\\\n                                  sum, rank, isnull, regexp_replace,sort_array, collect_list,struct,\\\n                                  slice, to_date,to_timestamp,arrays_overlap, length, slice, array_except\nfrom pyspark.sql.types import FloatType, StringType, StructField, StructType, IntegerType, DecimalType, NullType,BooleanType,DateType\nfrom pyspark.sql.functions import date_format\nfrom pyspark.sql import DataFrame\nfrom pyspark.ml.feature import HashingTF, IDF, MinHashLSH, NGram, RegexTokenizer\nfrom pyspark.ml.linalg import SparseVector, VectorUDT\nfrom itertools import chain\nfrom functools import reduce\nimport math as m\nimport json, pprint\nfrom mlflow.tracking import MlflowClient\nfrom mlflow.entities.model_registry.model_version_status import ModelVersionStatus\nimport time\nimport uuid\nfrom decimal import Decimal"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Libraries","showTitle":true,"inputWidgets":{},"nuid":"ba7fe0e4-d7ed-464c-9c2f-726c92040d8a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["@udf(\"string\")\ndef nonnull_count(*argv):\n    \"\"\"return the number of nonnull and len > 0 values\"\"\"\n    _v = 0\n    for val in argv:\n        if val is not None:\n            _v+=1\n    return _v\n\n@udf(\"double\")\ndef weighted_score(*sims):  \n    # weight order: [FirstName, LastName, Street, City, State, Zip, Gender, DOB, Race, SSN, MBI]\n    weights = [\n      float(w_FName), float(w_MName), float(w_LName), \n      float(w_Street), float(w_City), float(w_County), float(w_State), float(w_Zip), \n      float(w_Gender), float(w_Dob), float(w_Age), float(w_Race), \n      float(w_SSN), float(w_MBI), float(w_Lic), float(w_StateDL)\n    ]\n    _running_weight_normalizer = float(0)\n    _running_score = float(0)\n    for idx, sim in enumerate(sims):\n        if sim is not None:\n            _running_score += float(sim * 100*weights[idx])\n            _running_weight_normalizer += float(weights[idx])\n    if _running_weight_normalizer==0:\n        return float(0)\n    else:\n        return _running_score / _running_weight_normalizer\n\n@pandas_udf(\"string\", PandasUDFType.GROUPED_AGG)\ndef lexical_last(v):\n    \"\"\"Sort groups lexically and return the last entry in the set\"\"\"\n    return v.max()\n\n@udf(\"string\")\ndef string_first(x, y):\n    \"\"\"return the string value that comes first alphabetically\"\"\"\n    return x if x < y else y\n\n@udf(\"string\")\ndef string_last(x, y):\n    \"\"\"return the string value that comes last alphabetically\"\"\"\n    return x if x >= y else y\n\n@udf(\"double\")\ndef cos_sim(x, y):\n    \"\"\"return the weighted cosine similarity between two vector columns\"\"\"\n    if x is not None and y is not None:\n        if x.norm(2)*y.norm(2) == 0:\n            return float(0)\n        else:\n            return float(x.dot(y)/(x.norm(2)*y.norm(2)))\n    \n@udf(\"double\")\ndef jaccard_sim(x, y):\n    \"\"\"return the weighted jaccard similarity between two vector columns\"\"\"\n    if x is not None and y is not None:\n        xi, yi = set(x.indices), set(y.indices)\n        xd, yd = dict(zip(xi, x.values)), dict(zip(yi, y.values))\n        sum1, sum2 = 0, 0\n        for idx in xi | yi:      \n            sum1 += m.pow(min(xd.get(idx,0), yd.get(idx,0)), 2)\n            sum2 += m.pow(max(xd.get(idx,0), yd.get(idx,0)), 2)\n        if m.sqrt(sum2)==0:\n            return float(0)\n        else:\n            return float(m.sqrt(sum1)/m.sqrt(sum2))\n\n@udf(\"double\")\ndef relative_numeric_sim(x, y):\n    \"\"\"return the relative numerical difference between single index spark vectors\"\"\"\n    if x is not None and y is not None:\n        age1 = x.indices[0]\n        age2 = y.indices[0]\n        denom = m.fabs(max([float(age1), float(age2)]))\n        if denom != 0:\n            return m.pow(1.0 - (m.fabs(float(age1) - float(age2)) / denom), 2)\n    else:\n        return float(0)\n\ndef drop_values_below_threshold(vec, threshold): \n    \"\"\"\n      Given a vector of token weights, drop all features below the provided weight-threshold.\n      The idea is that super-common tokens (SCT) should be given low weights (by IDF) and \n      SCTs will cause MinHashLSH trouble, as it will result in giant buckets/bins.\n      Further, SCTs don't even contribute much to similarities, by definition, since they have low weights.\n      So, we drop them here to avoid the MinHashLSH performance pitfalls\n    \"\"\"\n    #_dels is a list of indices to drop\n    _dels = [x for x,y in enumerate(vec.values) if y<threshold]\n    return SparseVector(\n      vec.size,   # vector size is unchanged, it is based on the global set of all tokens\n      [y for x,y in enumerate(vec.indices) if x not in _dels],  # these are the indexes we want to keep\n      [y for x,y in enumerate(vec.values) if x not in _dels]    # these are the values we want to keep\n    )\n\ndrop_values_below_threshold_udf = udf(drop_values_below_threshold, VectorUDT())\n\ndef add_hashes(df, id=\"sourceId\", var=\"variable\", val=\"value\"):\n    \"\"\"\n        valueId is shared by all tokens with the same value, e.g. 'Luke' and 'Luke' \n        df ~ dataframe to modify\n        id ~ column with original source ids\n        var ~ column with the name of the variable (e.g. original source column)\n        val ~ column with source values\n    \"\"\"\n    return (df\n            .withColumn(\"valueId\", sha2(lower(trim(col(val))), 256))\n            .withColumn(\"featureId\", sha2(concat(col(id), col(var)), 256))\n           )\n\ndef drop_bad_values(df, val=\"value\", vid=\"valueId\", fid=\"featureId\"):\n    \"\"\"\n        Given a dataframe, return a smaller dataframe where we drop all entries associated with\n        bad/ignorable values... like blanks\n        df ~ dataframe to modify\n        val ~ column with original value (not used currently)\n        vid ~ hashed version of val\n        fid ~ unique record identifier\n    \"\"\"\n    return df.filter(trim(col(val)) != \"\")\n\ndef dedup_values(df, val=\"value\", vid=\"valueId\", fid=\"featureId\"):\n    \"\"\"\n        Given a dataframe, return a smaller dataframe where we keep only 1 row per value.\n        We need to be deterministic in choosing which row to keep, so we call a custom function.\n\n        df ~ dataframe to modify\n        val ~ column with original value (not used currently)\n        vid ~ hashed version of val\n        fid ~ unique record identifier\n    \"\"\"\n    keepers = (df\n               .groupBy(vid).agg(lexical_last(fid))\n               .withColumnRenamed(f\"lexical_last({fid})\", fid)\n               .drop(vid)\n              )\n    return broadcast(keepers).join(df, fid, \"left\")\n\nvsize = udf(lambda v: float(v.norm(1)),FloatType())\n\ndef tokenize(df, mode=\"default\", iCol=\"value\", oCol=\"tokens\", id=\"featureId\", sid=None):\n    \"\"\"\n        Return dataframe with new column containing list of tokens for a given input string-like column\n\n        df ~ dataframe to modify\n        mode ~ tokenizing approach (default/bigram/trigram/numeric)\n        iCol ~ column containing the data to tokenize\n        oCol ~ column name to create and store the tokenized data\n        id ~ how to uniquely identify each row\n        sid ~ (option) source ID to pass through\n    \"\"\"\n    rcols = [id, oCol] if sid is None else [sid, id, oCol] #optionally return a sourceId pulled from the \"sid\" column (for default)\n    gcols = [id] if sid is None else [id, sid] #optionally return a sourceId pulled from the \"sid\" column (for ngram)\n    if mode==\"numeric\":\n        \"\"\"e.g. 1 -> [1]\"\"\"\n        return df.withColumn(oCol, array(iCol)).select(*rcols)\n    if mode==\"default\":\n        \"\"\"e.g. \"some cool words\" -> [\"some\", \"cool\", \"words\"]\"\"\"\n        tk = RegexTokenizer(inputCol=iCol, outputCol=oCol, pattern=\"\\\\W\")\n        return tk.transform(df).withColumn(oCol, array_distinct(col(oCol))).select(*rcols)\n    else:\n        tk = RegexTokenizer(inputCol=iCol, outputCol=\"temp1\", pattern=\"\\\\W\")\n    if mode==\"bigram\":\n        \"\"\"e.g. \"some cool words\" -> [\"_s\", \"so\", \"om\", \"me\", \"e_\", \"_c\", \"co\", \"oo\", .....] \"\"\"\n        ng = NGram(n=2, inputCol=\"temp1\", outputCol=oCol)\n    elif mode==\"trigram\":\n        \"\"\"e.g. \"some cool words\" -> [\"_so\", \"som\", \"ome\", \"me_\", \"_co\", \"coo\", \"ool\", .....]\"\"\"\n        ng = NGram(n=3, inputCol=\"temp1\", outputCol=oCol)\n    else:\n        return\n    \n    return (ng.transform(tk.transform(df)\n                         .withColumn(\"temp1\", explode(\"temp1\"))\n                         .withColumn(\"temp1\", array_distinct(split(\"temp1\", \"\")))\n                        )\n            .withColumn(oCol, explode(oCol))\n            .groupBy(*gcols)\n            .agg(collect_set(oCol).alias(oCol))\n            .filter(size(col(\"tokens\"))>0)\n           )\n\ndef spvec(x, vsize=262144):\n    \"\"\"This takes a number and creates a sparse vector where the index=number is 1\n         This is not, admittedly, not great solution to a generic numeric similarity approach\n         We are using this because we are not separating string from numeric features,\n           this means we need to make sure we return the same type for both: a sparse vector\n    \"\"\"\n    return SparseVector(vsize, [int(x[0]) % vsize], [1])\n\nspvec = udf(spvec, VectorUDT())\n\ndef numeric_featurize(df, iCol=\"tokens\", oCol=\"features\", id=\"featureId\", sid=None):\n    if sid is not None:\n        return df.withColumn(oCol, spvec(col(iCol))).select(id, sid, oCol)\n    else:\n        return df.withColumn(oCol, spvec(col(iCol))).select(id, oCol)\n\ndef featurize(df, iCol=\"tokens\", oCol=\"features\", id=\"featureId\", idf_threshold=4.0, sid=None, tf=True):\n    \"\"\"\n        Given a column containing arrays of tokens, measure the document frequency, weight the tokens and return as a sparse vector\n\n        iCol ~ column containing the tokenized form of the data, e.g. [\"some\", \"cool\", \"words\"]\n        oCol ~ this is the name of the column to create that will contain the output\n        id ~ column that uniquely identifies each row\n        idf_threshold ~ minimum token weight, anything below this will be dropped from the filtered_idfXform DF\n    \"\"\"\n    rcols = [id, oCol] if sid is None else [sid, id, oCol] #optionally return a sourceId pulled from the \"sid\" column\n    htf = HashingTF(inputCol=iCol, outputCol=f\"raw_{oCol}\")\n    fdata = htf.transform(df).select(id, f\"raw_{oCol}\") if sid is None else htf.transform(df).select(sid, id, f\"raw_{oCol}\")\n    if tf:\n        idf = IDF(inputCol=f\"raw_{oCol}\", outputCol=oCol)\n        idfModel = idf.fit(fdata)\n        full_idfXform = idfModel.transform(fdata).select(*rcols)\n    else:\n        full_idfXform = fdata.withColumnRenamed(f\"raw_{oCol}\", oCol)\n    if idf_threshold is not None:  # Its possible user desires tokenization/featurization, but no binning, so idf_threshold is None\n        filtered_idfXform = (full_idfXform\n                             .select(id, oCol)\n                             .withColumn(\"features\", drop_values_below_threshold_udf(col(\"features\"), lit(idf_threshold)))\n                             .filter(vsize(col(\"features\")) != 0)\n                            )\n    else:\n        filtered_idfXform = None\n    return filtered_idfXform, full_idfXform  # Return the filtered AND unfiltered result\n\ndef binning(df, iCol=\"features\", oCol=\"hashes\", dCol=\"minHashJaccardDistance\", id=\"featureId\", numHashes=10, threshold=0.5):\n    \"\"\"\n        This uses the MinHashLSH algorithm to generate candidate pairs based on a similarity threshold.\n\n        df ~ dataframe with data to generate candidate pairs\n        iCol ~ column name containing features for comparison approximations\n        oCol ~ where to store generated minhashes\n        dCol ~ column name to create containing the approximate jaccard distance between pairs\n        id ~ unique record id\n        numHashes ~ number of unique hash functions to use in MinHashLSH.  Using more yields higher precision, but requires more compute\n        threshold ~ equivalent to (1 - jaccardSimilarity); pairs must be at least this similar to become candidates, lower == more restrictive\n    \"\"\"\n    mh = MinHashLSH(inputCol=iCol, outputCol=oCol, numHashTables=numHashes)\n    mh_model = mh.fit(df)\n    return (mh_model\n            .approxSimilarityJoin(df, df, threshold, distCol=dCol)\n            .filter(col(f\"datasetA.{id}\")>col(f\"datasetB.{id}\"))\n            .select(col(f\"datasetA.{id}\").alias(f\"{id}1\"), col(f\"datasetB.{id}\").alias(f\"{id}2\"), dCol)\n           )\n\ndef normCols(DF):\n    \"\"\"For all columns in DF do the following:\n         - remove all leading whitespace characters\n         - remove all trailing whitespace characters\n         - change all letters to lower-case\n    \"\"\"\n    for c in DF.columns:\n        DF = DF.withColumn(c, lower(regexp_extract(col(c), r'\\s*(.*)\\s*', 1)))\n    return DF\n\ndef map_to_targets(df, mappings, context, source):\n    \"\"\"The contextConfig.sourceMappings part of the config file contains mappings that need to be applied.\n        Each mapping contains a source name ('deltaTable'), a list of source columns ('sourceCols') and, optionally, a target alias ('targetAlias')\n        This function is applied for a given context (e.g. Person Names) and a given source table.\n        For each relevant mapping found in the config file, this function adds a column that combines the sourceCols\n    \"\"\"\n    maps = mappings.filter((col(\"contextName\")==context) & (col(\"deltaTable\")==source)).select(\"sourceCols\",\"targetAlias\").collect()\n    to_drop = []\n    for _map in maps:\n        _c = _map[0]\n        _t = context + \"__\" + (_map[1] if _map[1] is not None else _c[0])\n        df = df.withColumn(_t, concat_ws(\" \", *[coalesce(c, lit(\"\")) for c in _c]))\n        to_drop += _c\n    return df.drop(*list(set(to_drop)))\n\ndef melt(df, id_vars, value_vars, var_name=\"variable\", value_name=\"value\"):\n    # adapated from https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe\n    \"\"\"Convert DataFrame from wide to long format.\"\"\"\n    _vars_and_vals = create_map(\n        list(chain.from_iterable([\n            [lit(c), col(c)] for c in value_vars]\n        ))\n    )\n    _tmp = df.select(*id_vars, explode(_vars_and_vals)) \\\n        .withColumnRenamed('key', var_name) \\\n        .withColumnRenamed('value', value_name)\n\n    return _tmp\n\ndef convertToNull(dfa):\n    for i in dfa.columns:\n        dfa = dfa.withColumn(i , when(trim(col(i)) == '', None ).otherwise(col(i)))\n    return dfa\n  \n# Wait until the model is ready\ndef wait_until_ready(model_name, model_version):\n    client = MlflowClient()\n    for _ in range(60):\n        model_version_details = client.get_model_version(\n          name=model_name,\n          version=model_version,\n        )\n        status = ModelVersionStatus.from_string(model_version_details.status)\n        print(\"Model status: %s\" % ModelVersionStatus.to_string(status))\n        print('#' + '='*100)\n        if status == ModelVersionStatus.READY:\n            break\n        time.sleep(1)\n    \n    \ndef load_latest_model(model_name):\n    client = MlflowClient()\n    model_version_infos = client.search_model_versions(f\"name = '{model_name}'\")\n    max_model_version = max([int(model_version_info.version) for model_version_info in model_version_infos])\n    print(f'The latest version of pm-gbt model is:  {max_model_version}')\n    print('#' + '='*100)\n  \n    model_uri = f\"models:/{model_name}/{max_model_version}\"\n    print(f\"Model uri is {model_uri}\")\n    print('#' + '='*100)\n  \n    wait_until_ready(model_name, max_model_version)\n  \n    model = mlflow.spark.load_model(model_uri)\n    return model\n\n\nuuid_udf = udf(lambda: str(uuid.uuid4()), StringType()).asNondeterministic()\n\n#user = dbutils.secrets.get(\"snowflake\", \"snowflake-databricks-username\")\n#password = dbutils.secrets.get(\"snowflake\", \"snowflake-databricks-pw\")\n#database = dbutils.secrets.get(\"snowflake\", \"snowflake-databricks-database\")\n#url = dbutils.secrets.get(\"snowflake\", \"snowflake-host\")\n\n# snowflake connection options\n#options = {\"sfUrl\": url,\n #          \"sfUser\": user,\n #          \"sfPassword\": password,\n #          \"sfDatabase\": database,\n #          \"sfSchema\": \"ET3_LOOKER\",\n  #         \"truncate_table\" : \"ON\",\n  #         \"usestagingtable\" : \"OFF\"\n  #          }\n\ndef write_sf_data(df, table_name):\n    ''' Function to write data from a dataframe in databricks to snowflake\n        df = pyspark.dataframe\n        table_name = snowflake table name in this format [schema].[tablename]\n            ie. = ET3_LOOKER.MATCHED\n    '''\n    df.write\\\n    .format(\"snowflake\") \\\n    .options(**options) \\\n    .option(\"dbtable\", table_name) \\\n    .mode('overwrite')\\\n    .save()\n\ndef load_sf_data(table):\n    data = spark.read.format(\"snowflake\").options(**options).option(\"dbtable\", table).load()\n#   data.createOrReplaceTempView(table)\n    return data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Functions","showTitle":true,"inputWidgets":{},"nuid":"a87457a3-3dfc-4b3d-8480-4bf83935a75b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"00_functions_and_libraries","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2631527172704805}},"nbformat":4,"nbformat_minor":0}
