{"cells":[{"cell_type":"code","source":["%run ./00_functions_and_libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import required libraries and define functions","showTitle":true,"inputWidgets":{},"nuid":"b73a96bc-b922-4564-9b46-37d548487685"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run ./01_params"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Creates dict containing required variables, filepaths etc.","showTitle":true,"inputWidgets":{},"nuid":"536f2335-99bc-4d2a-8e46-a05687c6d12c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: &#39;if params[\\&#39;Database\\&#39;] == &#34;prod&#34;:\\n  params[\\&#39;RF_LOC\\&#39;] = &#34;X&#34;\\nelif params[\\&#39;Database\\&#39;] == &#34;impl&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;Database\\&#39;]==&#34;val&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;db\\&#39;]==&#34;dev1&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;Database\\&#39;] == &#34;dev0&#34;:\\n  params[\\&#39;RF_LOC\\&#39;]\\nelse:\\n  raise ValueError(\\&#39;{} is invalid or unmapped\\&#39;.format(db))&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: &#39;if params[\\&#39;Database\\&#39;] == &#34;prod&#34;:\\n  params[\\&#39;RF_LOC\\&#39;] = &#34;X&#34;\\nelif params[\\&#39;Database\\&#39;] == &#34;impl&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;Database\\&#39;]==&#34;val&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;db\\&#39;]==&#34;dev1&#34;:\\n  params[\\&#39;Database\\&#39;]\\nelif params[\\&#39;Database\\&#39;] == &#34;dev0&#34;:\\n  params[\\&#39;RF_LOC\\&#39;]\\nelse:\\n  raise ValueError(\\&#39;{} is invalid or unmapped\\&#39;.format(db))&#39;</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set('spark.sql.execution.arrow.enabled', True)\nspark.conf.set('spark.sql.execution.arrow.fallback.enabled', False)\nspark.conf.set(\"spark.sql.session.timeZone\", \"America/New_York\")\nspark.conf.set(\n  params[\"AzureSASLocation\"],\n  dbutils.secrets.get(scope=params[\"AzureSASScope\"],key=params[\"AzureSASKey\"])\n)\n\n\ndb = params[\"Database\"]\ncheckpoint = params[\"sparkCheckpointDir_claims\"]\ncfgfile = params['ClaimConfigFile']\nintDataDir = params[\"intermediateDataDir_claims\"]\ntempDataDir = params[\"tempdir_claims\"]\noutboundDir = None #params[\"AzureET3Mount\"]+\"prod/outbound\"\narchiveDir = None #params[\"AzureET3Mount\"]+\"prod/archive/outbound\"\nsnowflakeDir = None #params[\"AzureET3Mount\"]+\"prod/snowflake/\"\n\nspark.sql(f\"create database if not exists {db}\")\nspark.sql(f\"use {db}\")\nspark.sparkContext.setCheckpointDir(checkpoint)\nprint(f\"Input Parameters:\\n   Database: {db}\\n   Spark Checkpoint Dir: {checkpoint}\\n   Weights Config File:{cfgfile}\\n   Intermediate Dir:{intDataDir}\\n   Temp Dir:{tempDataDir}\\n   Outbound Dir:{outboundDir}\\n   Archive Dir:{archiveDir}\\n   Snowflake Dir:{snowflakeDir}\")\nassert \"ml\" in params['sparkVersion'], \"Spark ML runtime is a requirement!\"\nassert int(params['sparkVersion'].split(\".\")[0]) >= 8, \"Spark version 8 or above is a requirement!\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Configs","showTitle":true,"inputWidgets":{},"nuid":"81a93767-1e05-40ff-8656-9da20c603540"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Input Parameters:\n   Database: prod\n   Spark Checkpoint Dir: /mnt/edfr/et3/prod/HelperData_claims/checkpoint\n   Weights Config File:wasbs://et3@edfrvaproddbrstoragegen2.blob.core.usgovcloudapi.net/prod/HelperData/Claim_Batch_Dedup_config_sample_data.json\n   Intermediate Dir:/mnt/edfr/et3/prod/HelperData_claims\n   Temp Dir:/mnt/edfr/et3/prod/HelperData_claims/temp\n   Outbound Dir:None\n   Archive Dir:None\n   Snowflake Dir:None\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Input Parameters:\n   Database: prod\n   Spark Checkpoint Dir: /mnt/edfr/et3/prod/HelperData_claims/checkpoint\n   Weights Config File:wasbs://et3@edfrvaproddbrstoragegen2.blob.core.usgovcloudapi.net/prod/HelperData/Claim_Batch_Dedup_config_sample_data.json\n   Intermediate Dir:/mnt/edfr/et3/prod/HelperData_claims\n   Temp Dir:/mnt/edfr/et3/prod/HelperData_claims/temp\n   Outbound Dir:None\n   Archive Dir:None\n   Snowflake Dir:None\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["w_LName = params[\"WEIGHT: Last Name\"]\nw_FName = params[\"WEIGHT: First Name\"]\nw_MName = params[\"WEIGHT: Middle Initial/Name\"]\nw_Street = params[\"WEIGHT: Patient's Home Address\"]\nw_City = params[\"WEIGHT: Patient's Home City\"]\nw_County = params[\"WEIGHT: Patient's Home County\"]\nw_State = params[\"WEIGHT: Patient's Home State\"]\nw_Zip = params[\"WEIGHT: Patient's Home ZIP Code\"]\nw_SSN = params[\"WEIGHT: Social Security Number\"]\nw_Gender = params[\"WEIGHT: Gender\"]\nw_Race = params[\"WEIGHT: Race\"]\nw_Age = params[\"WEIGHT: Age\"]\nw_Dob = params[\"WEIGHT: Date of Birth\"]\nw_Lic = params[\"WEIGHT: Driver's License Number\"]\nw_MBI = params[\"WEIGHT: MBI\"]\nw_StateDL = params[\"WEIGHT: State Issuing Driver's License\"]\n\nw_total_active = 0\nfor x in [w_LName, w_FName, w_MName,w_Street, w_City, w_County, w_State, w_Zip,w_Gender, w_Race, w_Dob, w_Age, w_Lic, w_StateDL, w_SSN,w_MBI]:\n  w_total_active=+float(x)\n\nmatch_threshold = params[\"Match Threshold\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create Variable Weighting","showTitle":true,"inputWidgets":{},"nuid":"48af1082-4741-4758-8bb7-204f698bcff1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#######################\n\"\"\"PARSE CONFIG FILE\"\"\"\n#######################\nconfig = spark.read.option(\"multiline\", True).json(cfgfile)\n\n# \"Mappings\" tell us how to take data from the source columns and map them to our logical contexts for matching\n#    e.g. concat the f_name and l_name columns from source1 and map that to PersonNames\n#           and take full_name from source2 and map that to PersonNames.\nmappings = (config\n            .select(explode(\"contextConfig\").alias(\"contextConfig\"))\n            .withColumn(\"contextName\", col(\"contextConfig.contextName\"))\n            .withColumn(\"sourceMappings\", explode(\"contextConfig.sourceMappings\"))\n            .withColumn(\"deltaTable\", col(\"sourceMappings.deltaTable\"))\n            .withColumn(\"sourceCols\", col(\"sourceMappings.sourceCols\"))\n            .withColumn(\"targetAlias\", col(\"sourceMappings.targetAlias\"))\n            .drop(\"contextConfig\", \"sourceMappings\")\n           )\n\n# \"Sources\" are the delta tables containing the source data to be matched\n#   This configuration will tell us:\n#        - what the existing primary key is\n#        - whether to look for duplicates within the source (i.e. selfDedup = True/False)\nsources = (config\n           .select(explode(\"sourceConfig\").alias(\"sourceConfig\"))\n           .withColumn(\"deltaTable\", col(\"sourceConfig.deltaTable\"))\n           .withColumn(\"primaryKey\", col(\"sourceConfig.primaryKey\"))\n           .withColumn(\"selfDedup\", col(\"sourceConfig.selfDedup\"))\n           .drop(\"sourceConfig\")\n          )\n\n# \"Tokenizers\" are methods for matching.  We can tokenize by spaces/words, or by ngram.\n#    This config is set per matching context.  So, if you want to match on PersonNames and BusinessNames, there should be an entry for both\n#    We also need to know how to set up the MinHash algorithm for each context\ntokenizers = (config\n              .select(explode(\"contextConfig\").alias(\"contextConfig\"))\n              .withColumn(\"contextName\", col(\"contextConfig.contextName\"))\n              .withColumn(\"tokenizerConfig\", explode(\"contextConfig.tokenizerConfig\"))\n              .withColumn(\"mode\", col(\"tokenizerConfig.mode\"))\n              .withColumn(\"jaccardIndexThreshold\", col(\"tokenizerConfig.binningConfig.jaccardIndexThreshold\"))\n              .withColumn(\"numHashTables\", col(\"tokenizerConfig.binningConfig.numHashTables\"))\n              .withColumn(\"idfCutoff\", col(\"tokenizerConfig.binningConfig.idfCutoff\"))\n              .withColumn(\"termFreq\", col(\"tokenizerConfig.tf\"))\n              .drop(\"contextConfig\", \"tokenizerConfig\")\n             )\n\nsources.show(5)\ntokenizers.show(20)\nmappings.show(100, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Parse Entity Resolution Config File","showTitle":true,"inputWidgets":{},"nuid":"3c095780-a6d1-4a87-af45-8be5d4ec2661"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+-------------+---------+\n|deltaTable|   primaryKey|selfDedup|\n+----------+-------------+---------+\n|  pcr_meta|      [mp_id]|    false|\n| claims_pm|[CLM_UNIQ_ID]|    false|\n+----------+-------------+---------+\n\n+--------------+-------+---------------------+-------------+---------+--------+\n|   contextName|   mode|jaccardIndexThreshold|numHashTables|idfCutoff|termFreq|\n+--------------+-------+---------------------+-------------+---------+--------+\n|    PersonName|default|                  0.3|            4|      2.0|    null|\n|    PersonName|trigram|                 null|         null|     null|    null|\n|     FirstName|trigram|                 null|         null|     null|    null|\n|      LastName|trigram|                 null|         null|     null|    null|\n|           SSN|default|                 0.05|            4|     0.01|    null|\n|       Address|default|                  0.3|            4|      5.1|    null|\n|Address_street|trigram|                 null|         null|     null|    null|\n|  Address_city|trigram|                 null|         null|     null|    null|\n| Address_state|trigram|                 null|         null|     null|   false|\n|   Address_zip|default|                 null|         null|     null|    null|\n|        Gender|default|                 null|         null|     null|   false|\n|           DOB|default|                 null|         null|     null|    null|\n|          Race|default|                 null|         null|     null|   false|\n|    MiddleName|trigram|                 null|         null|     null|    null|\n|Address_county|trigram|                 null|         null|     null|    null|\n|           Age|numeric|                 null|         null|     null|    null|\n|           MBI|default|                 null|         null|     null|    null|\n+--------------+-------+---------------------+-------------+---------+--------+\n\n+--------------+----------+------------------------------------+--------------+\n|contextName   |deltaTable|sourceCols                          |targetAlias   |\n+--------------+----------+------------------------------------+--------------+\n|PersonName    |pcr_meta  |[first_name, last_name]             |null          |\n|PersonName    |claims_pm |[BENE_1ST_NAME, BENE_LAST_NAME]     |null          |\n|FirstName     |pcr_meta  |[first_name]                        |null          |\n|FirstName     |claims_pm |[BENE_1ST_NAME]                     |null          |\n|LastName      |pcr_meta  |[last_name]                         |null          |\n|LastName      |claims_pm |[BENE_LAST_NAME]                    |null          |\n|SSN           |pcr_meta  |[ssn]                               |null          |\n|SSN           |claims_pm |[BENE_SSN_NUM]                      |null          |\n|Address       |pcr_meta  |[address, city, zip_code]           |primary       |\n|Address       |claims_pm |[BENE_LINE_1_ADR, City, SRC_ZIP5_CD]|primary       |\n|Address_street|pcr_meta  |[address]                           |primary_street|\n|Address_street|claims_pm |[BENE_LINE_1_ADR]                   |primary_street|\n|Address_city  |pcr_meta  |[city]                              |null          |\n|Address_city  |claims_pm |[City]                              |null          |\n|Address_state |pcr_meta  |[state]                             |null          |\n|Address_state |claims_pm |[SRC_USPS_STATE_CD]                 |null          |\n|Address_zip   |pcr_meta  |[zip_code]                          |null          |\n|Address_zip   |claims_pm |[SRC_ZIP5_CD]                       |null          |\n|Gender        |pcr_meta  |[gender]                            |null          |\n|Gender        |claims_pm |[BENE_SEX_CD]                       |null          |\n|DOB           |pcr_meta  |[dob]                               |null          |\n|DOB           |claims_pm |[bene_BRTH_DT]                      |null          |\n|Race          |pcr_meta  |[race]                              |null          |\n|Race          |claims_pm |[BENE_RACE_CD]                      |null          |\n|MiddleName    |pcr_meta  |[MIDDLE_NAME]                       |null          |\n|MiddleName    |claims_pm |[BENE_MIDL_NAME]                    |null          |\n|Address_county|pcr_meta  |[county]                            |null          |\n|Address_county|claims_pm |[County]                            |null          |\n|Age           |pcr_meta  |[AGE]                               |null          |\n|Age           |claims_pm |[Age]                               |null          |\n|MBI           |pcr_meta  |[MBI]                               |null          |\n|MBI           |claims_pm |[BENE_MBI_ID]                       |null          |\n+--------------+----------+------------------------------------+--------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------+---------+\ndeltaTable|   primaryKey|selfDedup|\n+----------+-------------+---------+\n  pcr_meta|      [mp_id]|    false|\n claims_pm|[CLM_UNIQ_ID]|    false|\n+----------+-------------+---------+\n\n+--------------+-------+---------------------+-------------+---------+--------+\n   contextName|   mode|jaccardIndexThreshold|numHashTables|idfCutoff|termFreq|\n+--------------+-------+---------------------+-------------+---------+--------+\n    PersonName|default|                  0.3|            4|      2.0|    null|\n    PersonName|trigram|                 null|         null|     null|    null|\n     FirstName|trigram|                 null|         null|     null|    null|\n      LastName|trigram|                 null|         null|     null|    null|\n           SSN|default|                 0.05|            4|     0.01|    null|\n       Address|default|                  0.3|            4|      5.1|    null|\nAddress_street|trigram|                 null|         null|     null|    null|\n  Address_city|trigram|                 null|         null|     null|    null|\n Address_state|trigram|                 null|         null|     null|   false|\n   Address_zip|default|                 null|         null|     null|    null|\n        Gender|default|                 null|         null|     null|   false|\n           DOB|default|                 null|         null|     null|    null|\n          Race|default|                 null|         null|     null|   false|\n    MiddleName|trigram|                 null|         null|     null|    null|\nAddress_county|trigram|                 null|         null|     null|    null|\n           Age|numeric|                 null|         null|     null|    null|\n           MBI|default|                 null|         null|     null|    null|\n+--------------+-------+---------------------+-------------+---------+--------+\n\n+--------------+----------+------------------------------------+--------------+\ncontextName   |deltaTable|sourceCols                          |targetAlias   |\n+--------------+----------+------------------------------------+--------------+\nPersonName    |pcr_meta  |[first_name, last_name]             |null          |\nPersonName    |claims_pm |[BENE_1ST_NAME, BENE_LAST_NAME]     |null          |\nFirstName     |pcr_meta  |[first_name]                        |null          |\nFirstName     |claims_pm |[BENE_1ST_NAME]                     |null          |\nLastName      |pcr_meta  |[last_name]                         |null          |\nLastName      |claims_pm |[BENE_LAST_NAME]                    |null          |\nSSN           |pcr_meta  |[ssn]                               |null          |\nSSN           |claims_pm |[BENE_SSN_NUM]                      |null          |\nAddress       |pcr_meta  |[address, city, zip_code]           |primary       |\nAddress       |claims_pm |[BENE_LINE_1_ADR, City, SRC_ZIP5_CD]|primary       |\nAddress_street|pcr_meta  |[address]                           |primary_street|\nAddress_street|claims_pm |[BENE_LINE_1_ADR]                   |primary_street|\nAddress_city  |pcr_meta  |[city]                              |null          |\nAddress_city  |claims_pm |[City]                              |null          |\nAddress_state |pcr_meta  |[state]                             |null          |\nAddress_state |claims_pm |[SRC_USPS_STATE_CD]                 |null          |\nAddress_zip   |pcr_meta  |[zip_code]                          |null          |\nAddress_zip   |claims_pm |[SRC_ZIP5_CD]                       |null          |\nGender        |pcr_meta  |[gender]                            |null          |\nGender        |claims_pm |[BENE_SEX_CD]                       |null          |\nDOB           |pcr_meta  |[dob]                               |null          |\nDOB           |claims_pm |[bene_BRTH_DT]                      |null          |\nRace          |pcr_meta  |[race]                              |null          |\nRace          |claims_pm |[BENE_RACE_CD]                      |null          |\nMiddleName    |pcr_meta  |[MIDDLE_NAME]                       |null          |\nMiddleName    |claims_pm |[BENE_MIDL_NAME]                    |null          |\nAddress_county|pcr_meta  |[county]                            |null          |\nAddress_county|claims_pm |[County]                            |null          |\nAge           |pcr_meta  |[AGE]                               |null          |\nAge           |claims_pm |[Age]                               |null          |\nMBI           |pcr_meta  |[MBI]                               |null          |\nMBI           |claims_pm |[BENE_MBI_ID]                       |null          |\n+--------------+----------+------------------------------------+--------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#########################\n\"\"\"CONFIGURING SOURCES\"\"\"\n#########################\nsourceList = []  # Each element of this list will be a Dataframe of a source Delta table\nsourceCount = len(sources.select(\"deltaTable\").collect())  # Loop once for each source Delta table in config file\n\n# Lets get a list of all of the columns needed from each delta table\n# If a column isn't listed in the mappings, then we don't explicitly need it.\nallColumns = (mappings\n              .withColumn(\"sourceCols\", explode(\"sourceCols\"))\n              .groupBy(\"deltaTable\")\n              .agg(collect_set(\"sourceCols\").alias(\"columns\"))\n             )\n\nprint(\"\\nReading source delta tables into dataframes...\")\nfor sNum in range(sourceCount):\n  # There will be 1 iteration of this loop per source table. \n  # sName/sKey/sCol ==> Name/PrimaryKey/ColumnsNamesAsString\n  sName = allColumns.select(\"deltaTable\").rdd.collect()[sNum][0]\n  sKey = sources.filter(col(\"deltaTable\")==sName).select(\"primaryKey\").collect()[0][0]\n  sCol = \", \".join(sKey + allColumns.select(\"columns\").rdd.collect()[sNum][0])\n  source = spark.sql(f\"select {sCol} from {sName} where er_flag = 1\").fillna(\"\")\n  assert source.select(sKey[0]).distinct().count() == source.count(), f\"Error: Specified primaryKey \\\"{sKey[0]}\\\" for source \\\"{sName}\\\" is not unique\"\n  source = source.withColumn(sKey[0], concat_ws(\"__\", *[lit(sName), col(sKey[0])])) # sKey must be a single column\n  source = source.withColumnRenamed(sKey[0], \"sourceId\")\n  source = normCols(source)\n  sourceList.append(source)\n  print(f\"   Source '{sName}' is ready\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read Source Tables","showTitle":true,"inputWidgets":{},"nuid":"5d85a143-2ab1-42f7-9258-a0b4d41c103e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\nReading source delta tables into dataframes...\n   Source &#39;pcr_meta&#39; is ready\n   Source &#39;claims_pm&#39; is ready\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\nReading source delta tables into dataframes...\n   Source &#39;pcr_meta&#39; is ready\n   Source &#39;claims_pm&#39; is ready\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["##########################\n\"\"\"CONFIGURING CONTEXTS\"\"\"\n##########################\n\n# There will be 1 entry per context, keyed on the context name in the following two dictionaries\n# The dictionary values will be dataframes containing the context values and the source names & record ids they originated from\ncontextDict = {}     # Dataframes in this list are deduped so there is one row per value in each context.\ncontextDictRaw = {}  # Dataframes in this list are not deduped\n# Q: Why dedup in the step above?\n# A: If we have 1000 records that all contain the value \"some cool words\" and 50 records with the value \"some cooler words\", \n#    we shouldn't compare \"some cool words\" to \"some cooler words\" 50,000 times. Instead, we just do it once and keep track of all\n#    the other records that share the same value\n\ncontextNames = mappings.select(\"contextName\").distinct()\ncontextCount = contextNames.count()  # We will be looping over all of the context in the contextConfig section of the config file\ncontextColumns = (mappings  # For each context, let's only keep the source columns that we need.\n                  .withColumn(\"sourceCols\", explode(\"sourceCols\"))\n                  .groupBy(\"contextName\", \"deltaTable\")\n                  .agg(collect_set(\"sourceCols\").alias(\"columns\"))\n                 )\n\n# We are doing a lot of up-front deduplication so as to minimize the work done during MinHashLSH. \n# We need a method for undoing the deduplication afterward - \n#    e.g. if 2 records are identical, keep one and MinHash it, then for every candidate pair it generates, \n#         create a second candidate pair using the record we dropped earlier\n# So, we store the full list of ALL records and what values they contained\n# We'll join back on this table after doing the MinHash approach.\nschema = StructType([\n  StructField(\"sourceId\", StringType()), \n  StructField(\"featureId\", StringType()), \n  StructField(\"valueId\", StringType()),\n  StructField(\"contextName\", StringType())\n])\nsource_to_feature = spark.createDataFrame([], schema)\n\n# 1 loop per context\nfor cNum in range(contextCount):\n  cName = contextNames.collect()[cNum][0]\n  thisContext = []\n  thisContextRaw = []\n  for sNum in range(sourceCount): # 1 inner loop per source delta table\n    sName = allColumns.select(\"deltaTable\").rdd.collect()[sNum][0]   # Get the delta table name\n    cCol = (contextColumns\n            .filter((col(\"contextName\")==cName) & (col(\"deltaTable\")==sName))\n            .select(\"columns\")\n           ).collect()[0][0]  # Get the list of the required columns\n    _context = sourceList[sNum].select([\"sourceId\"] + cCol)  # keep only the required columns\n    _context = map_to_targets(_context, mappings, cName, sName)  # map the source columns to our target columns\n    _context = melt(_context, [\"sourceId\"], _context.columns[1:])  # pivot the data so we get 1 row per context value\n    # Note, in the above step, this pivot is necessary, because 1 source could have multiple representations of the same context\n    #    e.g. full_name, known_alias, also_known_as, etc.  OR home_address, work_address, alt_address, etc.\n    #    So 1 source row could produce more than 1 row in our context table... 1 row per context value.\n    \n    _context = drop_bad_values(add_hashes(_context)).withColumn(\"contextName\", lit(cName))\n    thisContextRaw.append(_context)\n    thisContext.append(dedup_values(_context).checkpoint())  # Here we dedup on the values to minimize work during MinHash\n    # Note, in the above step we are checkpointing here because otherwise we higt an NPE at the tokenize() step\n    \n  combinedContext = reduce(DataFrame.unionAll, thisContext)  # Combine this contexts data from all of the sources\n  combinedContextRaw = reduce(DataFrame.unionAll, thisContextRaw)  # Same thing as above but for non-deduped data\n  contextDict[cName] = combinedContext\n  contextDictRaw[cName] = combinedContextRaw\n  combinedContextRaw.write.format(\"delta\").mode(\"overwrite\").save(f\"{intDataDir}/delta/SILVER/context_{cName}\")\n  combinedContextRaw = spark.read.format(\"delta\").load(f\"{intDataDir}/delta/SILVER/context_{cName}\")\n  \n  # This is our lookup table to get back to the original record granularity\n  source_to_feature = source_to_feature.union(combinedContextRaw.select(\"sourceId\", \"featureId\", \"valueId\", \"contextName\"))\n\nsource_to_feature.createOrReplaceTempView(\"source_to_feature\")\n\nfor k,v in contextDict.items():\n  print(\"context info\")\n  print(f\"  name: {k}\")\n  print(f\"  record count: {v.count()}\")\n  print(\". example: \")\n  try:\n    pprint.pprint(v.take(1)[0].asDict())\n  except IndexError:\n    print(Exception(f\"\\n Warning: The Context Table {k} is empty. It will not contribute to probabilistic matching.\"))\n  print(\"\\n\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Construct Logical Contexts for Matching","showTitle":true,"inputWidgets":{},"nuid":"4547cb09-bb69-406c-b2c8-f5f5e771d1b8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#######################\n\"\"\"CONFIGURING PAIRS\"\"\"\n#######################\n\n# featuresList is a list of dictionaries; 1 dict per tokenization strategy\n#    each dictionary will contain the contextName, the tokenizer method, and a mapping of sourceIds to vectorized/weighted features\nfeaturesList = []\n\n# pairsList is a list of Dataframes containing the candidate pairs generated from each tokenization/binning strategy\n#    each dataframe in this list contains just 2 columns: id1, id2 (where these ids each identify a unique value for the context)\n#      e.g. The DF may say that featureId 55 forms a candidate pair with featureId 145.  \n#           We can go back to the source_to_feature table to see that featureId 55 == \"some cool words\" and featureId 145 == \"some cooler words\"\npairsList = []\n\nip_schema = StructType([StructField(\"sourceId1\", StringType()), StructField(\"sourceId2\", StringType())])\nident_pairs = spark.createDataFrame([], ip_schema)\n\nfor TCFG in tokenizers.collect():\n  # iterating over each tokenization strategy.  \n  #   TCFG has the following schemao\n  #      element 0: contextName (string)\n  #      element 1: mode (string)\n  #      element 2: jaccardIndexThreshold (double)\n  #      element 3: numHashTables (long)\n  #      element 4: idfCutoff (double)\n  #      element 5: termFreq (boolean)\n  _termFreq = True if TCFG[5] is None else False\n  \n  # Here we finally tokenize our data so, for instance, \"some cool words\" becomes [\"some\", \"cool\", \"words\"]\n  # Note we are dropping rows where the tokens column is empty\n  _tokens = tokenize(contextDict.get(TCFG[0]), mode=TCFG[1], sid=\"sourceId\").filter(size(col(\"tokens\"))>0)\n  if _tokens.count() == 0:\n    continue\n  \n  # Here we convert the token arrays into sparse vectors with IDF weights per token\n  #   Note, the featurize function returns 2 Dataframes, the first has been filtered to remove insignficant tokens (below the idfThreshold)\n  #         the second contains vectors with ALL tokens (we need these for accurate similarity functions during scoring later)\n  #   Also Note, the full dataframe is not presently needed, so we discard it \n  _sigFeatures, junk = featurize(_tokens, idf_threshold=TCFG[4], sid=\"sourceId\", tf=_termFreq)\n  \n  # We are tokenizing again because we also need a version here where we did NOT dedup all of our values.\n  #   We need this so we can properly reconstruct all of the candidate pairs later.\n  _tokensALL = tokenize(contextDictRaw.get(TCFG[0]), mode=TCFG[1], sid=\"sourceId\").filter(size(col(\"tokens\"))>0)\n  \n  # This time, all we care about is the DataFrame with all tokens\n  # Similarly, the filtered version of this dataframe is not needed at this time, so we discard it \n  if TCFG[1]==\"numeric\":\n    _allFeatures = numeric_featurize(_tokensALL, sid=\"sourceId\")\n  else:\n    junk, _allFeatures = featurize(_tokensALL, idf_threshold=TCFG[4], sid=\"sourceId\", tf=_termFreq)\n  \n  # Accumulate our results into featuresList\n  featuresList.append({\"name\":TCFG[0], \"tokenizer\":TCFG[1], \"fullFeatures\":_allFeatures})\n  \n  # It is possible that a tokenizer is set up, but no binning is desired.\n  # So, if a jaccardIndexThreshold has been set, let's assume user wants to do binning.\n  if TCFG[2] is not None:\n    _pairs = binning(_sigFeatures, threshold=TCFG[2], numHashes=TCFG[3]).drop(\"minHashJaccardDistance\")\n    #print(f\"size of pairs for {TCFG[0]} is {_pairs.count()}\")\n    pairsList.append(_pairs)\n    \n    \"\"\"\n    We eliminated exact matches earlier and all we are getting from binning now are fuzzy matches.  \n    In more other words... if:\n      record1.text = \"some cool words\" \n      record2.text = \"some cool words\"\n      record3.text = \"some cooler words\"\n    Binning will generate one candidate pair between 2:3 (or 1:3 but not both).  We also need 1:2, which we are doing here.\n    We self-joining source_to_feature to find exact matches within each context\n    \"\"\"\n    thisContextData = source_to_feature.filter(col(\"contextName\")==TCFG[0]).select(\"sourceId\", \"valueId\")\n    thisContextIdentPairs1 = thisContextData.withColumnRenamed(\"sourceId\",\"sourceId1\")\n    thisContextIdentPairs2 = thisContextData.withColumnRenamed(\"sourceId\",\"sourceId2\")\n\n    thisContextIdentPairs = (thisContextIdentPairs1.join(thisContextIdentPairs2, \"valueId\", \"inner\")\n                             .filter(col(\"sourceId1\")!=col(\"sourceId2\"))\n                             .drop(\"valueId\")\n                            )\n    ident_pairs = ident_pairs.union(thisContextIdentPairs)\n    #print(f\"ident_pairs size after {TCFG[0]} is {ident_pairs.count()}\")\n\n\"\"\"Let's accumulate all of the candidate pairs from all of the binning here into one big dataframe\n   We are still just looking at 2 columns: featureId1, featureId2\n\"\"\"\nall_pairs = reduce(DataFrame.unionAll, pairsList)\n\n\"\"\"So, we have featureId1:featureId2, but we need to get back to sourceIds.  It will require 2 hops.\n   Let's get to valueId1:valueId2 (remember source_to_feature maps all featureIds back to all of their original valueIds and sourceIds)\n\"\"\"\nall_pairs = (all_pairs\n             .join(source_to_feature, all_pairs.featureId1==source_to_feature.featureId)\n             .withColumnRenamed(\"valueId\", \"valueId1\")\n             .withColumnRenamed(\"contextName\", \"contextName1\")\n             .drop(\"sourceId\", \"featureId1\", \"featureId\")\n             .join(source_to_feature, all_pairs.featureId2==source_to_feature.featureId)\n             .withColumnRenamed(\"valueId\", \"valueId2\")\n             .withColumnRenamed(\"contextName\", \"contextName2\")\n             .drop(\"sourceId\", \"featureId2\", \"featureId\")\n            )\n\n\"\"\"Now we can join with source_to_feature again to get all sourceId::sourceId\n   But we need to make sure we are always using matching Contexts.  \n     e.g. an Address value and Address_street value could match, but we wouldn't want to generate a pair here\n   Finally, after doing this series of joins, we should be back to our original record granularity, having undone all of the earlier\n      optimization dedups.\n\"\"\"\nall_pairs = (all_pairs\n             .join(\n               source_to_feature,\n               (all_pairs.valueId1==source_to_feature.valueId) & (all_pairs.contextName1==source_to_feature.contextName) \n             )\n             .withColumnRenamed(\"sourceId\", \"sourceId1\")\n             .drop(\"valueId\", \"valueId1\", \"featureId\", \"contextName\", \"contextName1\")\n             .join(\n               source_to_feature, \n               (all_pairs.valueId2==source_to_feature.valueId) & (all_pairs.contextName2==source_to_feature.contextName)\n             )\n             .withColumnRenamed(\"sourceId\", \"sourceId2\")\n             .drop(\"valueId\", \"valueId2\", \"featureId\", \"contextName\", \"contextName2\")\n            )\n\n\"\"\" Combine all fuzzy pairs with all exact pairs \"\"\"\n\nall_pairs = all_pairs.union(ident_pairs)\n\n\"\"\"Rearrange IDs so they are in deterministic (lexical order), for deduping\n   If we don't do this step that it's possible we have ID1:ID2 AND ID2:ID1.  We need to sort them so that a dedup will catch them.\n   This would be harder if all_pairs had more columns... but it's just 2 ID columns.  We'll join back in the actual features later.\n\"\"\" \nall_pairs = (all_pairs\n             .withColumn(\"sourceID1_temp\", string_first(col(\"sourceId1\"),col(\"sourceId2\")))\n             .withColumn(\"sourceID2_temp\", string_last(col(\"sourceId1\"),col(\"sourceId2\")))\n             .withColumn(\"sourceId1\", col(\"sourceID1_temp\"))\n             .withColumn(\"sourceId2\", col(\"sourceID2_temp\"))\n             .drop(\"sourceID1_temp\",\"sourceID2_temp\")\n            ).dropDuplicates([\"sourceId1\", \"sourceId2\"]).filter(col(\"sourceId1\")!=col(\"sourceId2\"))\n\n\"\"\"Drop pairs from within the same source unless selfDedup is configured\n  If we are looking for matches across more than 1 dataset, we have the option of searching for dupes\n    across AND within datasets, or just across.\n  We have to loop over each source here because it can be configured differently for each source.\n\"\"\"\nfor SRC in sources.collect():\n  if not SRC[2]: #If selfDedup is False\n    all_pairs = all_pairs.filter(~((col(\"sourceID1\").contains(SRC[0])) & (col(\"sourceID2\").contains(SRC[0]))))\n    \nall_pairs.write.format(\"delta\").mode(\"overwrite\").save(f\"{intDataDir}/delta/SILVER/all_pairs\")\nall_pairs = spark.read.format(\"delta\").load(f\"{intDataDir}/delta/SILVER/all_pairs\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Using MinHashLSH","showTitle":true,"inputWidgets":{},"nuid":"8749c9c6-33e7-4531-be32-4def1acd9e5a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(f\"Total pairs to be considered: {all_pairs.count()} \\nTotal Claims being considered: {all_pairs.select('sourceId1').distinct().count()} \\nTotal PCR_Metas being considered: {all_pairs.select('sourceId2').distinct().count()}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Candidate Pair Count","showTitle":true,"inputWidgets":{},"nuid":"15372918-a201-4c46-a6b4-6dbaed017016"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#################\n\"\"\"SCORE PAIRS\"\"\"\n#################\n\n\"\"\"Join back in the original features so we can actually score the potential pairs\"\"\"\nscored_pairs = all_pairs.select(\"sourceId1\",\"sourceId2\")\nscoreColumnsList = []\nrcols = []\n\n# This loops over our actual context features\n# The \"fullFeatures\" key:val pair within each element of featuresList has the schema: {sourceId, featureId, feature_vector}\n# We can join on sourceID now and get the feature_vector for both sides of each candidate pair\n# Then we can calculate jaccard and cosine similarities.\nfor featureInfo in featuresList:\n  _thisName = featureInfo.get(\"name\")\n  _thisTokenizer = featureInfo.get(\"tokenizer\")\n  _thisFeature = featureInfo.get(\"fullFeatures\").select(\"sourceId\",\"features\")\n  scoreColumn = f\"{_thisName}__{_thisTokenizer}\"\n  feature1 = f\"{scoreColumn}__features1\"\n  feature2 = f\"{scoreColumn}__features2\"\n  cosCol = f\"{scoreColumn}__cosine_sim\"\n  numCol = f\"{scoreColumn}_sim\"\n  \n  #Note: There is a subtle thing happening here.\n  #  First, and straightforwardly, we attach the feature vectors and use them to calculate the similarities.\n  #  But afterward we do a groupBy on sourceId pairs again, so as to keep the max value of each similarity\n  #  This is needed because the earlier joins could be cartesianing our pairs again.\n  #  e.g. \n  #.    there may only be one candidate pair for ID1 and ID2,\n  #     but ID1 could have two different feature vectors for the same context \n  #     (remember we may have mapped more than 1 column to a single context)\n  #     so, maybe ID1 has two names - \"Lucas\" AND \"Luke\" - while ID2 only has one - \"Luke\"\n  #     After this join we will again have 2 scored records for that one pair ID1:ID2:0% and ID1:ID2:100%.\n  #     and, obviously, we keep the higher score.\n  scored_pairs = (scored_pairs\n                   .join(broadcast(_thisFeature), scored_pairs.sourceId1 == _thisFeature.sourceId, how=\"left\")\n                   .withColumnRenamed(\"features\", feature1)\n                   .select(\"sourceId1\", \"sourceId2\", *rcols, feature1)\n                   .join(broadcast(_thisFeature), scored_pairs.sourceId2 == _thisFeature.sourceId, how=\"left\")\n                   .withColumnRenamed(\"features\", feature2)\n                   .select(\"sourceId1\", \"sourceId2\", *rcols, feature1, feature2)\n                  )\n  \n  if _thisTokenizer == \"numeric\":\n    scored_pairs = (scored_pairs\n                    .withColumn(numCol, round(relative_numeric_sim(feature1, feature2), 3))\n                    .groupBy(\"sourceId1\",\"sourceId2\")\n                    .max(*rcols, numCol)\n                   )\n    rcols += [numCol]\n                    \n  else:\n    scored_pairs = (scored_pairs\n                    .withColumn(cosCol, round(cos_sim(col(feature1), col(feature2)), 3))\n                    .groupBy(\"sourceId1\",\"sourceId2\")\n                    .max(*rcols, cosCol)\n                   )\n    rcols += [cosCol]\n\n  for _col in rcols:\n    scored_pairs = scored_pairs.withColumnRenamed(f\"Max({_col})\", _col)\n    \nscored_pairs.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{intDataDir}/delta/GOLD/scored_pairs\")\nscored_pairs = spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/scored_pairs\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Calculate Similarities","showTitle":true,"inputWidgets":{},"nuid":"fe57bf57-8c0e-4f8b-ad9c-40046c31ad56"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# This list should contain all of our numeric independent variables to be used as features\nkeepers = [\n  \"FirstName__trigram__cosine_sim\",\n  \"MiddleName__trigram__cosine_sim\",\n  \"LastName__trigram__cosine_sim\",\n  \"Address_street__trigram__cosine_sim\",\n  \"Address_city__trigram__cosine_sim\",\n  \"Address_county__trigram__cosine_sim\",\n  \"Address_state__trigram__cosine_sim\",\n  \"Address_zip__default__cosine_sim\",\n  \"Gender__default__cosine_sim\",\n  \"DOB__default__cosine_sim\",\n  \"Age__numeric_sim\",\n  \"Race__default__cosine_sim\",\n  \"SSN__default__cosine_sim\",\n  \"MBI__default__cosine_sim\",\n  \"DriverLicNum__default__cosine_sim\",\n  \"StateIssDriverLic__default__cosine_sim\"\n]\n\n# For purpose of weighted scores, we only care about our similarities and IDs\nfor keeper in keepers:\n  if keeper not in scored_pairs.columns:\n    scored_pairs = scored_pairs.withColumn(keeper, lit(None))\nweighted_input = scored_pairs.select(keepers + [\"sourceID1\", \"sourceID2\"])\n\nMATCH_THRESHOLD = float(match_threshold)\n\npredictions = (weighted_input\n               .withColumn(\n                 \"pm_score\", \n                 weighted_score(\n                   col(\"FirstName__trigram__cosine_sim\"),\n                   col(\"MiddleName__trigram__cosine_sim\"),\n                   col(\"LastName__trigram__cosine_sim\"),\n                   col(\"Address_street__trigram__cosine_sim\"),\n                   col(\"Address_city__trigram__cosine_sim\"),\n                   col(\"Address_county__trigram__cosine_sim\"),\n                   col(\"Address_state__trigram__cosine_sim\"),\n                   col(\"Address_zip__default__cosine_sim\"),\n                   col(\"Gender__default__cosine_sim\"),\n                   col(\"DOB__default__cosine_sim\"),\n                   col(\"Age__numeric_sim\"),\n                   col(\"Race__default__cosine_sim\"),\n                   col(\"SSN__default__cosine_sim\"),\n                   col(\"MBI__default__cosine_sim\"),\n                   col(\"DriverLicNum__default__cosine_sim\"),\n                   col(\"StateIssDriverLic__default__cosine_sim\")\n                 )\n               )\n              ).withColumn(\"prediction\", when(col(\"pm_score\")>=MATCH_THRESHOLD, lit(1)).otherwise(lit(0)))\npredictions.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{intDataDir}/delta/GOLD/predictions\")\npredictions=spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/predictions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Weighted Scores and Matches","showTitle":true,"inputWidgets":{},"nuid":"5dee4b79-a529-44b3-a6da-ea82d81fac03"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Is this where we should drop PM_SCORE < 85?\n\npm_thresh_below = predictions.filter(col(\"pm_score\") < 85)\npm_above = predictions.filter(col(\"pm_score\") >= 85)\n\n# Might be too early to implement?\nprint(f\"Total rows: {predictions.count()}\") \nprint(f\"Total rows with PM score > 85: {pm_above.count()}\")\nprint(f\"Total rows with pm score < 85: {pm_thresh_below.count()}\")\n# Looks like we filter for > 85 Threshold when creating the graph frames below"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4493230e-7853-487f-b1e1-848fa3524bc0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["'''Assign the appropriate patient ids to the claims from the PM predctions, using the below steps\n1.) Relabel the appropriate identifying fields for CLM_UNIQ_ID and mp_id, so we can grab relevant information from claims_master and pcr_master tables respectively.\n2.) Retrieve patient_ids and their respective dispatch_dates from pcr_master for the mp_ids in the claims-pcr_meta matches.\n3.) Retrieve CLM_FROM_DT from claims_master table for the claims.\n4.) Each claim-pcr_meta match has now been expanded to be a row for every patient_id associated with each mp_id in our matches.\n5.) Evaluate each line as a 'match' if they have both a positive 'prediction' value AND the CLM_FROM_DT matches the PCR's dispatch date\n'''\npredictions=spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/predictions\")\nwindow_spec = Window.partitionBy(col('sourceID1')).orderBy(col('pm_score').desc())\nclaim_predictions=predictions.withColumn(\"rank\",rank().over(window_spec)).orderBy(col('sourceID1'),col('rank').asc())\\\n.select('sourceID1','sourceID2','pm_score','prediction','rank')\\\n.withColumnRenamed('sourceID1','CLM_UNIQ_ID')\\\n.withColumn('CLM_UNIQ_ID',regexp_extract(col('CLM_UNIQ_ID'), '(.)(__)(\\w+)', 3))\\\n.withColumnRenamed('sourceID2','mp_id').withColumn('mp_id',upper(col('mp_id')))\\\n.withColumn('mp_id',regexp_extract(col('mp_id'), '(.)(__)(\\w+)', 3))\\\n.join(spark.sql('''SELECT \n                    mp_id,\n                    patient_id,\n                    pcr_dispatch_date \n                    FROM (\n                      SELECT mp_id,\n                      patient_id,\n                      cast(dispatch_timestamp as date) as pcr_dispatch_date, \n                      row_number() OVER (PARTITION BY mp_id,cast(dispatch_timestamp as date) ORDER BY patient_id DESC) rank \n                      FROM pcr_master\n                       ) tmp \n                     WHERE rank <= 1'''), on=\"mp_id\", how=\"inner\")\\\n.join(spark.sql(\"select CLM_UNIQ_ID, CLM_FROM_DT from claims_master\").withColumn('CLM_FROM_DT',to_date(col('CLM_FROM_DT'),'ddMMMyyyy')),on=\"CLM_UNIQ_ID\",how=\"inner\")\\\n.withColumn(\"match\",when((col(\"prediction\")==1) & (col(\"pcr_dispatch_date\")==col(\"CLM_FROM_DT\")), lit(1)).otherwise(lit(0)))\nclaim_predictions.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{intDataDir}/delta/GOLD/claim_predictions\")\n\nclaim_predictions=spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/claim_predictions\")\nclaim_predictions.createOrReplaceTempView('Matches')\n#This is for later validation and answers the questions: How many matches were there? How many matches also had the correct date? Of those without the correct date, what was the average days difference?\nmatch=spark.sql('''select sum(case when max_match=1 then 1 else 0 end) as match_count,\n              count(distinct case when max_pred=1 and max_match=0 then CLM_UNIQ_ID else null end) as pred_no_match_count,\n              count(distinct case when max_pred=0 then CLM_UNIQ_ID else null end) as no_pred_no_match_count--,\n              from (\n                Select CLM_UNIQ_ID, \n                max(prediction) as max_pred,\n                max(match) as max_match\n                from Matches group by 1) a''').collect()\n#these are claims that were thrown out at the minHash step as not having enough similarity with any PCRs to even be scored against them.\nhash_misses = spark.sql('select count(CLM_UNIQ_ID) from claims_master where CLM_UNIQ_ID not in (select distinct CLM_UNIQ_ID from Matches)').collect()[0][0]\nmatched=match[0][0] #claims that had a PM score >=85 and a date match to at least one claim\nnonMatch=match[0][1] #claims that had a PM score >=85 to at least one claim, but no date matches\nnonPred=match[0][2]+hash_misses #claims that did not have any PM scores >= 85\nprint(f'There are {matched} matches \\nThere are {nonMatch} non-matches\\nThere are {nonPred} non-preds')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28c74029-8145-4039-a6d3-e89e28a4167d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["claim_predictions=spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/claim_predictions\")\nclaim_predictions.createOrReplaceTempView('Matches')\nspark.sql('''\nselect\nCLM_UNIQ_ID as Claim_ID,\nmp_id,\npm_score,\npcr_dispatch_date,\nclm_from_dt,\ndays_diff\nfrom (\n        select CLM_UNIQ_ID, mp_id,pm_score, pcr_dispatch_date, clm_from_dt,\n        abs(datediff(clm_from_dt, pcr_dispatch_date)) as days_diff,\n        rank() over (partition by clm_uniq_id order by pm_score desc, abs(datediff(clm_from_dt, pcr_dispatch_date)) asc, mp_id) as match_rank\n        from matches \n        where clm_uniq_id in (\n                              Select CLM_UNIQ_ID\n                              from Matches \n                              group by 1\n                              having max(prediction)=0\n                              )\n        ) as a\nwhere match_rank = 1\n\nUnion\n\nselect CLM_UNIQ_ID,\n'' as mp_id,\n'' as pm_score,\n'' as pcr_dispatch_date,\n'' as clm_from_dt,\n'' as days_diff\nfrom claims_master \nwhere CLM_UNIQ_ID not in (select distinct CLM_UNIQ_ID from Matches)\n''').display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Claims without matches","showTitle":true,"inputWidgets":{},"nuid":"d4b9d3e7-f099-4307-a779-5970e9945ac2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect \nCLM_UNIQ_ID as Claim_ID,\nmp_id,\npatient_id,\npm_score,\npcr_dispatch_date,\nclm_from_dt,\ndays_diff\nfrom (\n select CLM_UNIQ_ID, mp_id,pm_score, pcr_dispatch_date, clm_from_dt,patient_id,\n        abs(datediff(clm_from_dt, pcr_dispatch_date)) as days_diff,\n        rank() over (partition by clm_uniq_id order by abs(datediff(clm_from_dt, pcr_dispatch_date)) asc, pm_score desc, mp_id) as match_rank\nfrom Matches\nwhere CLM_UNIQ_ID in (Select CLM_UNIQ_ID\n                      from Matches group by 1\n                      having max(prediction)=1 and max(match)=0)\nand pm_score >=85.0\n) a\nwhere match_rank = 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Claims that PM as Match but have no matching Date from ePCR","showTitle":true,"inputWidgets":{},"nuid":"0e2fd488-9365-4e94-86bc-c9b207169eba"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n--For claims that are non-matches, what is minimum days diff between service from date and dispatch date?\nselect CLM_UNIQ_ID, cast(min(abs(datediff(\n      case when prediction=1 and match=0 then clm_from_dt else null end,\n      case when prediction=1 and match=0 then pcr_dispatch_date else null end\n  ))) as int) as min_days_diff \nfrom Matches\nwhere CLM_UNIQ_ID in (Select CLM_UNIQ_ID\n                      from Matches group by 1\n                      having max(prediction)=1 and max(match)=0)\ngroup by 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Days Diff among Matches but no matching date","showTitle":true,"inputWidgets":{},"nuid":"bee620c3-e183-4af5-8de5-3bce6e1fbfb1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#There was one case observed, where a claim was being matched to two nearly identical pcrs, but who happened to have different patient ids, the below picks one at random and drops the unneeded helper columns\nspark.sql('select * from Matches').filter('match==1')\\\n.withColumn('window',rank().over(Window.partitionBy(col('CLM_UNIQ_ID')).orderBy(col('pm_score').desc(),col('patient_id').desc())))\\\n.filter('window==1').drop('rank','window','match','prediction')\\\n.createOrReplaceTempView('Matches')\nprint(\"There were {} claims matched to a PCR\".format(spark.sql('Select * from matches').count()))\nassert spark.sql('select CLM_UNIQ_ID, count(patient_id) from Matches group by 1 having count(patient_id)>1 order by CLM_UNIQ_ID'), \"Claims matching to multiple PCRs...\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11a9248a-d520-46c5-88f4-6c8cd372de04"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nMERGE INTO claims_master a using(\n                                select \n                                a.CLM_UNIQ_ID, a.CLM_TYPE_CD, a.Participant_NPI_1, a.Participant_NPI_source_1, a.Participant_NPI_2, a.Participant_NPI_source_2, a.CLM_FROM_DT, a.CLM_THRU_DT, a.geo_ptnt_pckp_sk,a.geo_drop_off_sk, a.CLM_FINL_ACTN_IND, a.DEMO_91_Flag, a.ET3_HCPC, a.HCPC_Modifier, a.CLM_SUBMSN_DT, a.BENE_MBI_ID, a.bene_BRTH_DT, a.BENE_LAST_NAME, a.BENE_1ST_NAME, a.BENE_MIDL_NAME,  a.BENE_LINE_1_ADR, a.BENE_LINE_2_ADR, a.BENE_LINE_3_ADR, a.BENE_LINE_4_ADR, a.BENE_LINE_5_ADR, a.BENE_LINE_6_ADR, a.SRC_USPS_STATE_CD, a.SRC_ZIP5_CD, a.BENE_SSN_NUM, a.BENE_SEX_CD, a.BENE_RACE_CD, b.patient_id\n                                from claims_master a\n                                left join Matches b\n                                on a.CLM_UNIQ_ID=b.CLM_UNIQ_ID) b\n  ON a.CLM_UNIQ_ID==b.CLM_UNIQ_ID\n  WHEN MATCHED \n    THEN UPDATE SET *\n  WHEN NOT MATCHED \n    THEN INSERT *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Update Claims Master table with patient ids.","showTitle":true,"inputWidgets":{},"nuid":"b0680a37-cb6c-4608-9eed-0fa770aff36a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.rm(f\"{tempDataDir}/claims_to_rf\", True)\nspark.sql('''select CLM_UNIQ_ID, Participant_NPI_1, CLM_FROM_DT, CLM_THRU_DT, geo_ptnt_pckp_sk, ET3_HCPC, HCPC_Modifier, CLM_SUBMSN_DT, BENE_MBI_ID, patient_id from claims_master''')\\\n.withColumn('CLM_FROM_DT',regexp_replace(to_date(col('CLM_FROM_DT'),'ddMMMyyyy').cast('String'),'-',''))\\\n.withColumn('CLM_THRU_DT',regexp_replace(to_date(col('CLM_THRU_DT'),'ddMMMyyyy').cast('String'),'-',''))\\\n.withColumn('CLM_SUBMSN_DT',regexp_replace(to_date(col('CLM_SUBMSN_DT'),'ddMMMyyyy').cast('String'),'-',''))\\\n.withColumnRenamed('CLM_SUBMSN_DT','Submission')\\\n.withColumnRenamed('patient_id','patientid')\\\n.coalesce(1).write.mode('overwrite').format('csv').option('header', 'true').option('delimiter', '|').option('emptyValue','').save(f\"{tempDataDir}/claims_to_rf\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Update claims table used for claims portion of the passback file to RF","showTitle":true,"inputWidgets":{},"nuid":"d98c1ba9-0367-4092-8a29-e65ed1a086fc"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"14_Claim_Batch_PM_Processing","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2261646131270778}},"nbformat":4,"nbformat_minor":0}
