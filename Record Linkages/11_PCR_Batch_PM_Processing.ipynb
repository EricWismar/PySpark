{"cells":[{"cell_type":"code","source":["%run ./00_functions_and_libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import required libraries and define functions","showTitle":true,"inputWidgets":{},"nuid":"b8c8ff94-334b-44d0-8e6f-cf97ccdce32c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./01_params"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Creates dict containing required variables, filepaths etc.","showTitle":true,"inputWidgets":{},"nuid":"ef099882-d733-4798-a2f4-c9e43b2c6307"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.conf.set('spark.sql.execution.arrow.enabled', True)\nspark.conf.set('spark.sql.execution.arrow.fallback.enabled', False)\nspark.conf.set(\"spark.sql.session.timeZone\", \"America/New_York\")\nspark.conf.set(\n  params[\"AzureSASLocation\"],\n  dbutils.secrets.get(scope=params[\"AzureSASScope\"],key=params[\"AzureSASKey\"])\n)\n\n\ndb = params[\"Database\"]\ncheckpoint = params[\"sparkCheckpointDir\"]\ncfgfile = params['erConfigFile']\nintDataDir = params[\"intermediateDataDir\"]\ntempDataDir = params[\"tempdir\"]\noutboundDir = params[\"AzureET3Mount\"]+\"prod/outbound\"\narchiveDir = params[\"AzureET3Mount\"]+\"prod/archive/outbound\"\nsnowflakeDir = params[\"AzureET3Mount\"]+\"prod/snowflake/\"\n\nspark.sql(f\"create database if not exists {db}\")\nspark.sql(f\"use {db}\")\nspark.sparkContext.setCheckpointDir(checkpoint)\nprint(f\"Input Parameters:\\n   Database: {db}\\n   Spark Checkpoint Dir: {checkpoint}\\n   Weights Config File:{cfgfile}\\n   Intermediate Dir:{intDataDir}\\n   Temp Dir:{tempDataDir}\\n   Outbound Dir:{outboundDir}\\n   Outbound Dir:{archiveDir}\\n   Outbound Dir:{snowflakeDir}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Environment Config","showTitle":true,"inputWidgets":{},"nuid":"06af1aff-6fe5-416c-b941-bffad1ceebbc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rf_write = params[\"RF_Write\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write RF arg","showTitle":true,"inputWidgets":{},"nuid":"4b4e5b46-6436-4aaa-bac0-ab5024c973e2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["w_LName = params[\"WEIGHT: Last Name\"]\nw_FName = params[\"WEIGHT: First Name\"]\nw_MName = params[\"WEIGHT: Middle Initial/Name\"]\nw_Street = params[\"WEIGHT: Patient's Home Address\"]\nw_City = params[\"WEIGHT: Patient's Home City\"]\nw_County = params[\"WEIGHT: Patient's Home County\"]\nw_State = params[\"WEIGHT: Patient's Home State\"]\nw_Zip = params[\"WEIGHT: Patient's Home ZIP Code\"]\nw_SSN = params[\"WEIGHT: Social Security Number\"]\nw_Gender = params[\"WEIGHT: Gender\"]\nw_Race = params[\"WEIGHT: Race\"]\nw_Age = params[\"WEIGHT: Age\"]\nw_Dob = params[\"WEIGHT: Date of Birth\"]\nw_Lic = params[\"WEIGHT: Driver's License Number\"]\nw_MBI = params[\"WEIGHT: MBI\"]\nw_StateDL = params[\"WEIGHT: State Issuing Driver's License\"]\n\nw_total_active = 0\nfor x in [w_LName, w_FName, w_MName,w_Street, w_City, w_County, w_State, w_Zip,w_Gender, w_Race, w_Dob, w_Age,  w_Lic, w_StateDL, w_SSN,w_MBI]:\n  w_total_active=+float(x)\n\nmatch_threshold = params[\"Match Threshold\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create variable weighting - CR1019","showTitle":true,"inputWidgets":{},"nuid":"13ede97b-5ee1-4318-9a52-5d9fbfc468ac"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#######################\n\"\"\"PARSE CONFIG FILE\"\"\"\n#######################\nconfig = spark.read.option(\"multiline\", True).json(cfgfile)\n\n# \"Mappings\" tell us how to take data from the source columns and map them to our logical contexts for matching\n#    e.g. concat the f_name and l_name columns from source1 and map that to PersonNames\n#           and take full_name from source2 and map that to PersonNames.\nmappings = (config\n            .select(explode(\"contextConfig\").alias(\"contextConfig\"))\n            .withColumn(\"contextName\", col(\"contextConfig.contextName\"))\n            .withColumn(\"sourceMappings\", explode(\"contextConfig.sourceMappings\"))\n            .withColumn(\"deltaTable\", col(\"sourceMappings.deltaTable\"))\n            .withColumn(\"sourceCols\", col(\"sourceMappings.sourceCols\"))\n            .withColumn(\"targetAlias\", col(\"sourceMappings.targetAlias\"))\n            .drop(\"contextConfig\", \"sourceMappings\")\n           )\n\n# \"Sources\" are the delta tables containing the source data to be matched\n#   This configuration will tell us:\n#        - what the existing primary key is\n#        - whether to look for duplicates within the source (i.e. selfDedup = True/False)\nsources = (config\n           .select(explode(\"sourceConfig\").alias(\"sourceConfig\"))\n           .withColumn(\"deltaTable\", col(\"sourceConfig.deltaTable\"))\n           .withColumn(\"primaryKey\", col(\"sourceConfig.primaryKey\"))\n           .withColumn(\"selfDedup\", col(\"sourceConfig.selfDedup\"))\n           .drop(\"sourceConfig\")\n          )\n\n# \"Tokenizers\" are methods for matching.  We can tokenize by spaces/words, or by ngram.\n#    This config is set per matching context.  So, if you want to match on PersonNames and BusinessNames, there should be an entry for both\n#    We also need to know how to set up the MinHash algorithm for each context\ntokenizers = (config\n              .select(explode(\"contextConfig\").alias(\"contextConfig\"))\n              .withColumn(\"contextName\", col(\"contextConfig.contextName\"))\n              .withColumn(\"tokenizerConfig\", explode(\"contextConfig.tokenizerConfig\"))\n              .withColumn(\"mode\", col(\"tokenizerConfig.mode\"))\n              .withColumn(\"jaccardIndexThreshold\", col(\"tokenizerConfig.binningConfig.jaccardIndexThreshold\"))\n              .withColumn(\"numHashTables\", col(\"tokenizerConfig.binningConfig.numHashTables\"))\n              .withColumn(\"idfCutoff\", col(\"tokenizerConfig.binningConfig.idfCutoff\"))\n              .withColumn(\"termFreq\", col(\"tokenizerConfig.tf\"))\n              .drop(\"contextConfig\", \"tokenizerConfig\")\n             )\n\nsources.show(5)\ntokenizers.show(20)\nmappings.show(100, False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Parse Entity Resolution Config File","showTitle":true,"inputWidgets":{},"nuid":"cbaf8d11-6ff8-4908-b5e4-adb7ac4b88ee"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#########################\n\"\"\"CONFIGURING SOURCES\"\"\"\n#########################\nsourceList = []  # Each element of this list will be a Dataframe of a source Delta table\nsourceCount = len(sources.select(\"deltaTable\").collect())  # Loop once for each source Delta table in config file\n\n# Lets get a list of all of the columns needed from each delta table\n# If a column isn't listed in the mappings, then we don't explicitly need it.\nallColumns = (mappings\n              .withColumn(\"sourceCols\", explode(\"sourceCols\"))\n              .groupBy(\"deltaTable\")\n              .agg(collect_set(\"sourceCols\").alias(\"columns\"))\n             )\n\nprint(\"\\nReading source delta tables into dataframes...\")\nfor sNum in range(sourceCount):\n  # There will be 1 iteration of this loop per source table. \n  # sName/sKey/sCol ==> Name/PrimaryKey/ColumnsNamesAsString\n  sName = allColumns.select(\"deltaTable\").rdd.collect()[sNum][0]\n  sKey = sources.filter(col(\"deltaTable\")==sName).select(\"primaryKey\").collect()[0][0]\n  sCol = \", \".join(sKey + allColumns.select(\"columns\").rdd.collect()[sNum][0])\n  source = spark.sql(f\"select {sCol} from {sName} where er_flag = 1\").fillna(\"\")\n  assert source.select(sKey[0]).distinct().count() == source.count(), f\"Error: Specified primaryKey \\\"{sKey[0]}\\\" for source \\\"{sName}\\\" is not unique\"\n  source = source.withColumn(sKey[0], concat_ws(\"__\", *[lit(sName), col(sKey[0])])) # sKey must be a single column\n  source = source.withColumnRenamed(sKey[0], \"sourceId\")\n  source = normCols(source)\n  sourceList.append(source)\n  print(f\"   Source '{sName}' is ready\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read Source Tables","showTitle":true,"inputWidgets":{},"nuid":"a77d95c7-4a8c-4320-b5f8-cea3b78a8e28"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["##########################\n\"\"\"CONFIGURING CONTEXTS\"\"\"\n##########################\n\n# There will be 1 entry per context, keyed on the context name in the following two dictionaries\n# The dictionary values will be dataframes containing the context values and the source names & record ids they originated from\ncontextDict = {}     # Dataframes in this list are deduped so there is one row per value in each context.\ncontextDictRaw = {}  # Dataframes in this list are not deduped\n# Q: Why dedup in the step above?\n# A: If we have 1000 records that all contain the value \"some cool words\" and 50 records with the value \"some cooler words\", \n#    we shouldn't compare \"some cool words\" to \"some cooler words\" 50,000 times. Instead, we just do it once and keep track of all\n#    the other records that share the same value\n\ncontextNames = mappings.select(\"contextName\").distinct()\ncontextCount = contextNames.count()  # We will be looping over all of the context in the contextConfig section of the config file\ncontextColumns = (mappings  # For each context, let's only keep the source columns that we need.\n                  .withColumn(\"sourceCols\", explode(\"sourceCols\"))\n                  .groupBy(\"contextName\", \"deltaTable\")\n                  .agg(collect_set(\"sourceCols\").alias(\"columns\"))\n                 )\n\n# We are doing a lot of up-front deduplication so as to minimize the work done during MinHashLSH. \n# We need a method for undoing the deduplication afterward - \n#    e.g. if 2 records are identical, keep one and MinHash it, then for every candidate pair it generates, \n#         create a second candidate pair using the record we dropped earlier\n# So, we store the full list of ALL records and what values they contained\n# We'll join back on this table after doing the MinHash approach.\nschema = StructType([\n  StructField(\"sourceId\", StringType()), \n  StructField(\"featureId\", StringType()), \n  StructField(\"valueId\", StringType()),\n  StructField(\"contextName\", StringType())\n])\nsource_to_feature = spark.createDataFrame([], schema)\n\n# 1 loop per context\nfor cNum in range(contextCount):\n  cName = contextNames.collect()[cNum][0]\n  thisContext = []\n  thisContextRaw = []\n  for sNum in range(sourceCount): # 1 inner loop per source delta table\n    sName = allColumns.select(\"deltaTable\").rdd.collect()[sNum][0]   # Get the delta table name\n    cCol = (contextColumns\n            .filter((col(\"contextName\")==cName) & (col(\"deltaTable\")==sName))\n            .select(\"columns\")\n           ).collect()[0][0]  # Get the list of the required columns\n    _context = sourceList[sNum].select([\"sourceId\"] + cCol)  # keep only the required columns\n    _context = map_to_targets(_context, mappings, cName, sName)  # map the source columns to our target columns\n    _context = melt(_context, [\"sourceId\"], _context.columns[1:])  # pivot the data so we get 1 row per context value\n    # Note, in the above step, this pivot is necessary, because 1 source could have multiple representations of the same context\n    #    e.g. full_name, known_alias, also_known_as, etc.  OR home_address, work_address, alt_address, etc.\n    #    So 1 source row could produce more than 1 row in our context table... 1 row per context value.\n    \n    _context = drop_bad_values(add_hashes(_context)).withColumn(\"contextName\", lit(cName))\n    thisContextRaw.append(_context)\n    thisContext.append(dedup_values(_context).checkpoint())  # Here we dedup on the values to minimize work during MinHash\n    # Note, in the above step we are checkpointing here because otherwise we higt an NPE at the tokenize() step\n    \n  combinedContext = reduce(DataFrame.unionAll, thisContext)  # Combine this contexts data from all of the sources\n  combinedContextRaw = reduce(DataFrame.unionAll, thisContextRaw)  # Same thing as above but for non-deduped data\n  contextDict[cName] = combinedContext\n  contextDictRaw[cName] = combinedContextRaw\n  combinedContextRaw.write.format(\"delta\").mode(\"overwrite\").save(f\"{intDataDir}/delta/SILVER/context_{cName}\")\n  combinedContextRaw = spark.read.format(\"delta\").load(f\"{intDataDir}/delta/SILVER/context_{cName}\")\n  \n  # This is our lookup table to get back to the original record granularity\n  source_to_feature = source_to_feature.union(combinedContextRaw.select(\"sourceId\", \"featureId\", \"valueId\", \"contextName\"))\n\nsource_to_feature.createOrReplaceTempView(\"source_to_feature\")\n\nfor k,v in contextDict.items():\n  print(\"context info\")\n  print(f\"  name: {k}\")\n  print(f\"  record count: {v.count()}\")\n  print(\". example: \")\n  try:\n    pprint.pprint(v.take(1)[0].asDict())\n  except IndexError:\n    print(Exception(f\"\\n Warning: The Context Table {k} is empty. It will not contribute to probabilistic matching.\"))\n  print(\"\\n\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Construct Logical Contexts for Matching","showTitle":true,"inputWidgets":{},"nuid":"ece83779-f09f-4681-b77a-9d1ac7847dfc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#######################\n\"\"\"CONFIGURING PAIRS\"\"\"\n#######################\n\n# featuresList is a list of dictionaries; 1 dict per tokenization strategy\n#    each dictionary will contain the contextName, the tokenizer method, and a mapping of sourceIds to vectorized/weighted features\nfeaturesList = []\n\n# pairsList is a list of Dataframes containing the candidate pairs generated from each tokenization/binning strategy\n#    each dataframe in this list contains just 2 columns: id1, id2 (where these ids each identify a unique value for the context)\n#      e.g. The DF may say that featureId 55 forms a candidate pair with featureId 145.  \n#           We can go back to the source_to_feature table to see that featureId 55 == \"some cool words\" and featureId 145 == \"some cooler words\"\npairsList = []\n\nip_schema = StructType([StructField(\"sourceId1\", StringType()), StructField(\"sourceId2\", StringType())])\nident_pairs = spark.createDataFrame([], ip_schema)\n\nfor TCFG in tokenizers.collect():\n  # iterating over each tokenization strategy.  \n  #   TCFG has the following schema\n  #      element 0: contextName (string)\n  #      element 1: mode (string)\n  #      element 2: jaccardIndexThreshold (double)\n  #      element 3: numHashTables (long)\n  #      element 4: idfCutoff (double)\n  #      element 5: termFreq (boolean)\n  _termFreq = True if TCFG[5] is None else False\n  \n  # Here we finally tokenize our data so, for instance, \"some cool words\" becomes [\"some\", \"cool\", \"words\"]\n  # Note we are dropping rows where the tokens column is empty\n  _tokens = tokenize(contextDict.get(TCFG[0]), mode=TCFG[1], sid=\"sourceId\").filter(size(col(\"tokens\"))>0)\n  if _tokens.count() == 0:\n    continue\n  \n  # Here we convert the token arrays into sparse vectors with IDF weights per token\n  #   Note, the featurize function returns 2 Dataframes, the first has been filtered to remove insignficant tokens (below the idfThreshold)\n  #         the second contains vectors with ALL tokens (we need these for accurate similarity functions during scoring later)\n  #   Also Note, the full dataframe is not presently needed, so we discard it \n  _sigFeatures, junk = featurize(_tokens, idf_threshold=TCFG[4], sid=\"sourceId\", tf=_termFreq)\n  \n  # We are tokenizing again because we also need a version here where we did NOT dedup all of our values.\n  #   We need this so we can properly reconstruct all of the candidate pairs later.\n  _tokensALL = tokenize(contextDictRaw.get(TCFG[0]), mode=TCFG[1], sid=\"sourceId\").filter(size(col(\"tokens\"))>0)\n  \n  # This time, all we care about is the DataFrame with all tokens\n  # Similarly, the filtered version of this dataframe is not needed at this time, so we discard it \n  if TCFG[1]==\"numeric\":\n    _allFeatures = numeric_featurize(_tokensALL, sid=\"sourceId\")\n  else:\n    junk, _allFeatures = featurize(_tokensALL, idf_threshold=TCFG[4], sid=\"sourceId\", tf=_termFreq)\n  \n  # Accumulate our results into featuresList\n  featuresList.append({\"name\":TCFG[0], \"tokenizer\":TCFG[1], \"fullFeatures\":_allFeatures})\n  \n  # It is possible that a tokenizer is set up, but no binning is desired.\n  # So, if a jaccardIndexThreshold has been set, let's assume user wants to do binning.\n  if TCFG[2] is not None:\n    _pairs = binning(_sigFeatures, threshold=TCFG[2], numHashes=TCFG[3]).drop(\"minHashJaccardDistance\")\n    #print(f\"size of pairs for {TCFG[0]} is {_pairs.count()}\")\n    pairsList.append(_pairs)\n    \n    \"\"\"\n    We eliminated exact matches earlier and all we are getting from binning now are fuzzy matches.  \n    In more other words... if:\n      record1.text = \"some cool words\" \n      record2.text = \"some cool words\"\n      record3.text = \"some cooler words\"\n    Binning will generate one candidate pair between 2:3 (or 1:3 but not both).  We also need 1:2, which we are doing here.\n    We self-joining source_to_feature to find exact matches within each context\n    \"\"\"\n    thisContextData = source_to_feature.filter(col(\"contextName\")==TCFG[0]).select(\"sourceId\", \"valueId\")\n    thisContextIdentPairs1 = thisContextData.withColumnRenamed(\"sourceId\",\"sourceId1\")\n    thisContextIdentPairs2 = thisContextData.withColumnRenamed(\"sourceId\",\"sourceId2\")\n\n    thisContextIdentPairs = (thisContextIdentPairs1.join(thisContextIdentPairs2, \"valueId\", \"inner\")\n                             .filter(col(\"sourceId1\")!=col(\"sourceId2\"))\n                             .drop(\"valueId\")\n                            )\n    ident_pairs = ident_pairs.union(thisContextIdentPairs)\n    #print(f\"ident_pairs size after {TCFG[0]} is {ident_pairs.count()}\")\n\n\"\"\"Let's accumulate all of the candidate pairs from all of the binning here into one big dataframe\n   We are still just looking at 2 columns: featureId1, featureId2\n\"\"\"\nall_pairs = reduce(DataFrame.unionAll, pairsList)\n\n\"\"\"So, we have featureId1:featureId2, but we need to get back to sourceIds.  It will require 2 hops.\n   Let's get to valueId1:valueId2 (remember source_to_feature maps all featureIds back to all of their original valueIds and sourceIds)\n\"\"\"\nall_pairs = (all_pairs\n             .join(source_to_feature, all_pairs.featureId1==source_to_feature.featureId)\n             .withColumnRenamed(\"valueId\", \"valueId1\")\n             .withColumnRenamed(\"contextName\", \"contextName1\")\n             .drop(\"sourceId\", \"featureId1\", \"featureId\")\n             .join(source_to_feature, all_pairs.featureId2==source_to_feature.featureId)\n             .withColumnRenamed(\"valueId\", \"valueId2\")\n             .withColumnRenamed(\"contextName\", \"contextName2\")\n             .drop(\"sourceId\", \"featureId2\", \"featureId\")\n            )\n\n\"\"\"Now we can join with source_to_feature again to get all sourceId::sourceId\n   But we need to make sure we are always using matching Contexts.  \n     e.g. an Address value and Address_street value could match, but we wouldn't want to generate a pair here\n   Finally, after doing this series of joins, we should be back to our original record granularity, having undone all of the earlier\n      optimization dedups.\n\"\"\"\nall_pairs = (all_pairs\n             .join(\n               source_to_feature,\n               (all_pairs.valueId1==source_to_feature.valueId) & (all_pairs.contextName1==source_to_feature.contextName) \n             )\n             .withColumnRenamed(\"sourceId\", \"sourceId1\")\n             .drop(\"valueId\", \"valueId1\", \"featureId\", \"contextName\", \"contextName1\")\n             .join(\n               source_to_feature, \n               (all_pairs.valueId2==source_to_feature.valueId) & (all_pairs.contextName2==source_to_feature.contextName)\n             )\n             .withColumnRenamed(\"sourceId\", \"sourceId2\")\n             .drop(\"valueId\", \"valueId2\", \"featureId\", \"contextName\", \"contextName2\")\n            )\n\n\"\"\" Combine all fuzzy pairs with all exact pairs \"\"\"\n\nall_pairs = all_pairs.union(ident_pairs)\n\n\"\"\"Rearrange IDs so they are in deterministic (lexical order), for deduping\n   If we don't do this step that it's possible we have ID1:ID2 AND ID2:ID1.  We need to sort them so that a dedup will catch them.\n   This would be harder if all_pairs had more columns... but it's just 2 ID columns.  We'll join back in the actual features later.\n\"\"\" \nall_pairs = (all_pairs\n             .withColumn(\"sourceID1_temp\", string_first(col(\"sourceId1\"),col(\"sourceId2\")))\n             .withColumn(\"sourceID2_temp\", string_last(col(\"sourceId1\"),col(\"sourceId2\")))\n             .withColumn(\"sourceId1\", col(\"sourceID1_temp\"))\n             .withColumn(\"sourceId2\", col(\"sourceID2_temp\"))\n             .drop(\"sourceID1_temp\",\"sourceID2_temp\")\n            ).dropDuplicates([\"sourceId1\", \"sourceId2\"]).filter(col(\"sourceId1\")!=col(\"sourceId2\"))\n\n\"\"\"Drop pairs from within the same source unless selfDedup is configured\n  If we are looking for matches across more than 1 dataset, we have the option of searching for dupes\n    across AND within datasets, or just across.\n  We have to loop over each source here because it can be configured differently for each source.\n\"\"\"\nfor SRC in sources.collect():\n  if not SRC[2]: #If selfDedup is False\n    all_pairs = all_pairs.filter(~((col(\"sourceID1\").contains(SRC[0])) & (col(\"sourceID2\").contains(SRC[0]))))\n    \nall_pairs.write.format(\"delta\").mode(\"overwrite\").save(f\"{intDataDir}/delta/SILVER/all_pairs\")\nall_pairs = spark.read.format(\"delta\").load(f\"{intDataDir}/delta/SILVER/all_pairs\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Using MinHashLSH","showTitle":true,"inputWidgets":{},"nuid":"9547201c-3117-4fdf-825b-84e9e50c3d09"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(f\"Total pairs to be considered: {all_pairs.count()}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Candidate Pair Count","showTitle":true,"inputWidgets":{},"nuid":"2b2d9f0b-c6b5-4531-82be-d5f4ccb2c3fb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#################\n\"\"\"SCORE PAIRS\"\"\"\n#################\n\n\"\"\"Join back in the original features so we can actually score the potential pairs\"\"\"\nscored_pairs = all_pairs.select(\"sourceId1\",\"sourceId2\")\nscoreColumnsList = []\nrcols = []\n\n# This loops over our actual context features\n# The \"fullFeatures\" key:val pair within each element of featuresList has the schema: {sourceId, featureId, feature_vector}\n# We can join on sourceID now and get the feature_vector for both sides of each candidate pair\n# Then we can calculate jaccard and cosine similarities.\nfor featureInfo in featuresList:\n  _thisName = featureInfo.get(\"name\")\n  _thisTokenizer = featureInfo.get(\"tokenizer\")\n  _thisFeature = featureInfo.get(\"fullFeatures\").select(\"sourceId\",\"features\")\n  scoreColumn = f\"{_thisName}__{_thisTokenizer}\"\n  feature1 = f\"{scoreColumn}__features1\"\n  feature2 = f\"{scoreColumn}__features2\"\n  cosCol = f\"{scoreColumn}__cosine_sim\"\n  numCol = f\"{scoreColumn}_sim\"\n  \n  #Note: There is a subtle thing happening here.\n  #  First, and straightforwardly, we attach the feature vectors and use them to calculate the similarities.\n  #  But afterward we do a groupBy on sourceId pairs again, so as to keep the max value of each similarity\n  #  This is needed because the earlier joins could be cartesianing our pairs again.\n  #  e.g. \n  #.    there may only be one candidate pair for ID1 and ID2,\n  #     but ID1 could have two different feature vectors for the same context \n  #     (remember we may have mapped more than 1 column to a single context)\n  #     so, maybe ID1 has two names - \"Lucas\" AND \"Luke\" - while ID2 only has one - \"Luke\"\n  #     After this join we will again have 2 scored records for that one pair ID1:ID2:0% and ID1:ID2:100%.\n  #     and, obviously, we keep the higher score.\n  scored_pairs = (scored_pairs\n                   .join(broadcast(_thisFeature), scored_pairs.sourceId1 == _thisFeature.sourceId, how=\"left\")\n                   .withColumnRenamed(\"features\", feature1)\n                   .select(\"sourceId1\", \"sourceId2\", *rcols, feature1)\n                   .join(broadcast(_thisFeature), scored_pairs.sourceId2 == _thisFeature.sourceId, how=\"left\")\n                   .withColumnRenamed(\"features\", feature2)\n                   .select(\"sourceId1\", \"sourceId2\", *rcols, feature1, feature2)\n                  )\n  \n  if _thisTokenizer == \"numeric\":\n    scored_pairs = (scored_pairs\n                    .withColumn(numCol, round(relative_numeric_sim(feature1, feature2), 3))\n                    .groupBy(\"sourceId1\",\"sourceId2\")\n                    .max(*rcols, numCol)\n                   )\n    rcols += [numCol]\n                    \n  else:\n    scored_pairs = (scored_pairs\n                    .withColumn(cosCol, round(cos_sim(col(feature1), col(feature2)), 3))\n                    .groupBy(\"sourceId1\",\"sourceId2\")\n                    .max(*rcols, cosCol)\n                   )\n    rcols += [cosCol]\n\n  for _col in rcols:\n    scored_pairs = scored_pairs.withColumnRenamed(f\"Max({_col})\", _col)\n    \nscored_pairs.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{intDataDir}/delta/GOLD/scored_pairs\")\nscored_pairs = spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/scored_pairs\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Calculate Similarities","showTitle":true,"inputWidgets":{},"nuid":"5be97ec7-80d9-44c2-9ac9-7016d017dbd2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# This list should contain all of our numeric independent variables to be used as features\nkeepers = [\n  \"FirstName__trigram__cosine_sim\",\n  \"MiddleName__trigram__cosine_sim\",\n  \"LastName__trigram__cosine_sim\",\n  \"Address_street__trigram__cosine_sim\",\n  \"Address_city__trigram__cosine_sim\",\n  \"Address_county__trigram__cosine_sim\",\n  \"Address_state__trigram__cosine_sim\",\n  \"Address_zip__default__cosine_sim\",\n  \"Gender__default__cosine_sim\",\n  \"DOB__default__cosine_sim\",\n  \"Age__numeric_sim\",\n  \"Race__default__cosine_sim\",\n  \"SSN__default__cosine_sim\",\n  \"MBI__default__cosine_sim\",\n  \"DriverLicNum__default__cosine_sim\",\n  \"StateIssDriverLic__default__cosine_sim\"\n]\n\n# For purpose of weighted scores, we only care about our similarities and IDs\nfor keeper in keepers:\n  if keeper not in scored_pairs.columns:\n    scored_pairs = scored_pairs.withColumn(keeper, lit(None))\nweighted_input = scored_pairs.select(keepers + [\"sourceID1\", \"sourceID2\"])\n\nMATCH_THRESHOLD = float(match_threshold)\n\npredictions = (weighted_input\n               .withColumn(\n                 \"pm_score\", \n                 weighted_score(\n                   col(\"FirstName__trigram__cosine_sim\"),\n                   col(\"MiddleName__trigram__cosine_sim\"),\n                   col(\"LastName__trigram__cosine_sim\"),\n                   col(\"Address_street__trigram__cosine_sim\"),\n                   col(\"Address_city__trigram__cosine_sim\"),\n                   col(\"Address_county__trigram__cosine_sim\"),\n                   col(\"Address_state__trigram__cosine_sim\"),\n                   col(\"Address_zip__default__cosine_sim\"),\n                   col(\"Gender__default__cosine_sim\"),\n                   col(\"DOB__default__cosine_sim\"),\n                   col(\"Age__numeric_sim\"),\n                   col(\"Race__default__cosine_sim\"),\n                   col(\"SSN__default__cosine_sim\"),\n                   col(\"MBI__default__cosine_sim\"),\n                   col(\"DriverLicNum__default__cosine_sim\"),\n                   col(\"StateIssDriverLic__default__cosine_sim\")\n                 )\n               )\n              ).withColumn(\"prediction\", when(col(\"pm_score\")>=MATCH_THRESHOLD, lit(1)).otherwise(lit(0)))\npredictions.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{intDataDir}/delta/GOLD/predictions\")\npredictions=spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/predictions\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Weighted Scores and Matches","showTitle":true,"inputWidgets":{},"nuid":"27e4af00-b3e2-479e-9bff-90e9e75e3206"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Is this where we should drop PM_SCORE < 85?\npm_thresh_below = predictions.filter(col(\"pm_score\") < 85)\npm_above = predictions.filter(col(\"pm_score\") >= 85)\n\n# Might be too early to implement?\nprint(f\"Total rows: {predictions.count()}\") \nprint(f\"Total rows with PM score > 85: {pm_above.count()}\")\nprint(f\"Total rows with pm score < 85: {pm_thresh_below.count()}\")\n# Looks like we filter for > 85 Threshold when creating the graph frames below"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dc2b077-9ffe-4ae6-b59f-74e99622437b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\"\"\"\nTo use GraphFrames.connectedComponents, we need to provide two pieces:\n  - nodes: A Dataframe containing an ID column that uniquely identifies every source record\n           These will be the nodes points in our graph\n  - edges: A Dataframe containing 3 columns - 2 ID columns and a match probability\n           The IDs should represent the left and right side of a pair\n           The IDs should match with IDs in the nodes Dataframe \n           The match probability should be between 0-1\n\"\"\"\n\n# To get our edges, we'll start with our predictions\n# Also for readability, I update the names\n# For now, I only want to consider very likely matches in my groups, so I keep only labels with >92% confidence\n# I scale the confidences so that they range from 0-1 instead of 0.92 to 1.00 (Not sure if this is necessary)\nedges = (predictions\n         .withColumn('src', regexp_extract(col('sourceID1'), '(.)(__)(\\w+)', 3))\n         .withColumn('dst', regexp_extract(col('sourceID2'), '(.)(__)(\\w+)', 3))\n         .filter(col(\"pm_score\")>=match_threshold)\n         .withColumn(\"tf_adjusted_match_prob\", col(\"pm_score\"))\n         .select(\"src\", \"dst\", \"tf_adjusted_match_prob\")\n        )\n\n# To get all of the nodes, combine all of the source IDs from the primary and delta batch patient tables\nall_data = spark.sql(\"Select * from PCR_Master\")\nnodes = all_data.selectExpr(\"patient_id as id\")\n\nfrom graphframes import GraphFrame\n\n# Build the GraphFrame\ng = GraphFrame(nodes, edges)\n\n# Identify linked nodes\ncc = g.connectedComponents().select(\"id\",\"component\")\n\n# Join results with original data for human readability\nresults = cc.join(all_data, cc.id==all_data.patient_id, \"right\").drop(\"id\").orderBy(\"component\",\"patient_id\")\n#display(results)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Build Latest Round of Groupings","showTitle":true,"inputWidgets":{},"nuid":"10e5f11a-3941-45cf-9f81-17012fa21262"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import arrays_overlap, col, collect_set, first, lit,  when\n\n# this is a list of sme-flagged ids, along with all records that should NOT be linked to the record\ndelinked_pairs = spark.sql(\"select flagged_id, delinked_partner_id from delinked_pairs\")\n\n# same as above but the delinked records have been accumulated into an array called \"delinked_partner_ids\"\ndelinked_records = delinked_pairs.groupBy(\"flagged_id\").agg(collect_set(\"delinked_partner_id\").alias(\"delinked_partner_ids\"))\n\n# This is a mapping of each sme-flagged id to the component/group number to which it belongs (after the last round of PM matching/groupings) \ncomponent_ids_to_review = (results\n                           .select(\"patient_id\", \"component\")\n                           .join(delinked_records, results.patient_id==delinked_records.flagged_id)\n                           .select(\"flagged_id\", \"component\")\n                           .withColumnRenamed(\"component\", \"component2\")\n                          )\n\n# This is the same as above, but with a new column called \"member_ids\" that contains an array of all record ids in the group\ncomponents_to_review = (component_ids_to_review\n                        .join(results.select(\"patient_id\", \"component\"), component_ids_to_review.component2 == results.component)\n                        .drop(\"component2\")\n                        .groupBy(\"component\", \"flagged_id\")\n                        .agg(collect_set(\"patient_id\").alias(\"member_ids\"))\n                       )\n\n# Here we join the components_to_review table with the delinked_records table and filter for cases when there IS overlap between the \n#   delinked_partner_ids array and the member_ids array\n# In other words, if the sme-flagged record currently belongs to a group that contains at least 1 specifically forbidden record, \n#   then we need to isolate the sme-flagged record\nids_to_isolate = (delinked_records\n                  .join(components_to_review, \"flagged_id\")\n                  .filter(arrays_overlap(col(\"delinked_partner_ids\"), col(\"member_ids\")))\n                  .select(\"flagged_id\")\n                  .withColumn(\"indicator\", lit(1))\n                 )\n\n# We need to move these IDs into brand new groups. We can do this by giving them unique component values.\n# We know the patient_id's are unique integers and the components are integers, too.\n# Let's get the largest current value of component then add the patient_id\n#max_component = results.select(max(\"component\")).collect()[0][0]\nresults.createOrReplaceTempView('results_temp')\nmax_component=spark.sql('select max(component) from results_temp').collect()[0][0]\n\n# Now we take our original results and join it with the ids_to_isolate table, adding on just the indicator (1) for when a record needs to be moved\n#   we use a .when().otherwise() call to update the component using the arithmetic outlined above when the indicator is 1\nresults = (results\n           .join(ids_to_isolate, results.patient_id == ids_to_isolate.flagged_id, \"left\")\n           .drop(\"flagged_id\")\n           .withColumn(\"component\", \n                       when(col(\"indicator\")==1, lit(max_component) + col(\"patient_id\"))\n                       .otherwise(col(\"component\"))\n                      )\n           .drop(\"indicator\")\n          )\n#display(results)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Apply user overrides with delinked_pairs table","showTitle":true,"inputWidgets":{},"nuid":"731c8387-a352-4df7-9ca6-517b03fc98da"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#score the null rate by row for relevant columns in the dataframe\n#col_list = ['first_name','last_name','address','city','state','zip_code','ssn','gender','race','dob','mbi']\n\nresults_with_nr = results.withColumn(\"nonnull_count\", nonnull_count(col(\"first_name\"), col(\"last_name\"), col(\"middle_name\"), col(\"address\"), col('city'), col('state'), col('zip_code'), col('ssn'), col('county'),col('gender'), col('race'), col('dob'), col('mbi'), col('age'), col('drivers_license_number'), col('state_issuing_drivers_license')))\n                                     \ngrouping_field = ['component']\nwindow_spec = Window.partitionBy(*grouping_field).orderBy(col('nonnull_count').cast(IntegerType()).desc(), col(\"source\").asc(), \n                                                          col('claims_update_timestamp').desc(), col('pcr_received_timestamp').desc(),\n                                                         col('dispatch_timestamp').desc(),col('patient_id').asc())\n\nresults_with_rank_index = results_with_nr.withColumn(\"rank\", rank().over(window_spec))\n\nresults_with_rank_index = results_with_rank_index.withColumn('mp_id', regexp_replace('mp_id','M','')) #slice the \"M\" off if it exists, added back in line below\n\nresults_pc = (results_with_rank_index\n              .withColumn('pc_flag', when(col('rank')==1,'P').otherwise('C'))\n              .withColumn('mp_id', concat(lit('M'),first(col(\"mp_id\")).over(window_spec)))\n             )\n#results_pc.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Identify Best Record within Each Group","showTitle":true,"inputWidgets":{},"nuid":"09a40078-a829-46a4-89e8-dc63729d051b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# We will do a self join on the grouped results to generate all possible pairs from each group\nt1 = results_pc.select(col(\"patient_id\").alias(\"patient_id1\"), col(\"mp_id\").alias(\"mp_id1\"), col(\"pc_flag\").alias(\"pc_flag1\"))\nt2 = results_pc.select(col(\"patient_id\").alias(\"patient_id2\"), col(\"mp_id\").alias(\"mp_id2\"), col(\"pc_flag\").alias(\"pc_flag2\"))\n\npred_lkp = (predictions.select(\"sourceId1\", \"sourceId2\", \"pm_score\")\n            .withColumn(\"sourceId1\", regexp_extract(col('sourceID1'), '(.)(__)(\\w+)', 3))\n            .withColumn(\"sourceId2\", regexp_extract(col('sourceID2'), '(.)(__)(\\w+)', 3))\n           )\n\n# However, within each group, we only care about pairs where the right side is the parent\n# Also, we have to rearrange the pair labels as the smaller id is always on the left side\n#   but we want to make it easy to tell which one was the child later, so keep the child pcr_id around\npairs_with_parent = (t1.join(t2, t1.mp_id1==t2.mp_id2)\n                     .filter(col(\"pc_flag2\")==\"P\")\n                     .filter(col(\"patient_id1\")!=col(\"patient_id2\"))\n                     .withColumn(\"sourceId1\", when(col(\"patient_id1\")<col(\"patient_id2\"), col(\"patient_id1\")).otherwise(col(\"patient_id2\")))\n                     .withColumn(\"sourceId2\", when(col(\"patient_id1\")<col(\"patient_id2\"), col(\"patient_id2\")).otherwise(col(\"patient_id1\")))\n                     .select(\"sourceId1\", \"sourceId2\", col(\"patient_id1\").alias(\"patient_id\"))\n                    )\n\n# Next, let's check if we already have a pm_score from the earlier weighted scoring step\nprev_scored = pairs_with_parent.join(pred_lkp, [\"sourceId1\", \"sourceId2\"], how=\"left\")  # these already have a pm_score\n\n# For those without a pm_score, we'll need to get the column similarities, then use the same weighted model as before\n# mp_lkp is a shortcut back to the pcr_id that represents the child record, regardless if sourceId1 and sourceId2 were rearranged\nmp_lkp = prev_scored.filter(col(\"pm_score\").isNull()).select(\"sourceId1\", \"sourceId2\", \"patient_id\")\nunscored = mp_lkp.select(\"sourceId1\", \"sourceId2\")\n\n# Get the features and similarities for those in the unscored group\n# This code is essentially a duplicate of a cell from above... this can definitely be generalized and cleaned up\nscoreColumnsList = []\nrcols = []\nfor featureInfo in featuresList:\n  _thisName = featureInfo.get(\"name\")\n  _thisTokenizer = featureInfo.get(\"tokenizer\")\n  _thisFeature = (featureInfo.get(\"fullFeatures\").select(\"sourceId\",\"features\")\n                  .withColumn(\"sourceId\", regexp_extract(col('sourceID'), '(.)(__)(\\w+)', 3))\n                 ).checkpoint()\n  scoreColumn = f\"{_thisName}__{_thisTokenizer}\"\n  feature1 = f\"{scoreColumn}__features1\"\n  feature2 = f\"{scoreColumn}__features2\"\n  cosCol = f\"{scoreColumn}__cosine_sim\"\n  numCol = f\"{scoreColumn}_sim\"\n\n  unscored = (unscored\n              .join(broadcast(_thisFeature), unscored.sourceId1 == _thisFeature.sourceId, how=\"left\")\n              .withColumnRenamed(\"features\", feature1)\n              .select(\"sourceId1\", \"sourceId2\", *rcols, feature1)\n              .join(broadcast(_thisFeature), unscored.sourceId2 == _thisFeature.sourceId, how=\"left\")\n              .withColumnRenamed(\"features\", feature2)\n              .select(\"sourceId1\", \"sourceId2\", *rcols, feature1, feature2)\n             )\n  \n  if _thisTokenizer == \"numeric\":\n    unscored = (unscored\n                .withColumn(numCol, round(relative_numeric_sim(col(feature1), col(feature2)), 3))\n                .groupBy(\"sourceId1\",\"sourceId2\")\n                .max(*rcols, numCol)\n               )\n    rcols += [numCol]\n                    \n  else:\n    unscored = (unscored\n                .withColumn(cosCol, round(cos_sim(col(feature1), col(feature2)), 3))\n                .groupBy(\"sourceId1\",\"sourceId2\")\n                .max(*rcols, cosCol)\n               )\n    rcols += [cosCol]\n\n  for _col in rcols:\n    unscored = unscored.withColumnRenamed(f\"Max({_col})\", _col)\n\n# Here we join back to mp_lkp to get the pcr_id back\nfor keeper in keepers:\n  if keeper not in unscored.columns:\n    unscored = unscored.withColumn(keeper, lit(0.0))\nunscored = (unscored\n            .join(mp_lkp, [\"sourceId1\",\"sourceId2\"], how=\"left\")\n            .select(keepers + [\"sourceID1\", \"sourceID2\", \"patient_id\"])\n           )\n\nPM_scores = (unscored\\\n             .withColumn(\n               \"pm_score\", \n               weighted_score(\n                 col(\"FirstName__trigram__cosine_sim\"),\n                 col(\"MiddleName__trigram__cosine_sim\"),\n                 col(\"LastName__trigram__cosine_sim\"),\n                 col(\"Address_street__trigram__cosine_sim\"),\n                 col(\"Address_city__trigram__cosine_sim\"),\n                 col(\"Address_county__trigram__cosine_sim\"),\n                 col(\"Address_state__trigram__cosine_sim\"),\n                 col(\"Address_zip__default__cosine_sim\"),\n                 col(\"Gender__default__cosine_sim\"),\n                 col(\"DOB__default__cosine_sim\"),\n                 col(\"Age__numeric_sim\"),\n                 col(\"Race__default__cosine_sim\"),\n                 col(\"SSN__default__cosine_sim\"),\n                 col(\"MBI__default__cosine_sim\"),\n                 col(\"DriverLicNum__default__cosine_sim\"),\n                 col(\"StateIssDriverLic__default__cosine_sim\")\n               )\n             )\n             .select(\"patient_id\", \"pm_score\")\n            )\n\n# Now we have all of the pm_scores for all children in two disctinc DFs...union them here\nPM_scores = PM_scores.union(prev_scored.filter(~col(\"pm_score\").isNull()).select(\"patient_id\", \"pm_score\"))\n\n# Enrich the previous grouping results (with parent identified) with the pm_scores\nresults_pc_pm = results_pc.drop(\"pm_score\").join(PM_scores, \"patient_id\", how=\"left\")\nresults_pc_pm.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{intDataDir}/delta/GOLD/results_pc_pm\")\nresults_pc_pm = spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/results_pc_pm\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Fill in missing PM_scores","showTitle":true,"inputWidgets":{},"nuid":"17489622-fb47-4184-acb2-f57fe7ad94a2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import monotonically_increasing_id, row_number\nfrom pyspark.sql import Window\n\n# Only need to run if there are scores with less than 85 pm threshold\nif results_pc_pm.filter(col('pm_score') < 85).count() > 0:\n  # We need to find all rows that have a pm_score < 85 and update them to singletons\n  # First we separate the < 85 rows from the rest\n  under_threshold = results_pc_pm.filter(col('pm_score') < 85)\n  over_threshold = results_pc_pm.filter((col('pm_score') >= 85) | (col(\"pm_score\").isNull()))\n\n\n  # For all rows with a threshold < 85 we need to update them to singletons\n  # This involves:\n  # removing the mp_id\n  # Updating Child flag to P\n  # Updating component field to a single component\n  # updating mp_id to match M + patient id\n\n\n  # # Updating C flag to P\n  under_threshold = under_threshold.withColumn('pc_flag', lit('P'))\n\n  # # Update mp_id to match patient_id for singleton\n  under_threshold = under_threshold.withColumn('mp_id', concat(lit('M'),col(\"patient_id\")))\n\n  # # drop component to re-add it later\n  under_threshold = under_threshold.drop(\"component\")\n\n  # # DROP PM SCORE\n  under_threshold = under_threshold.withColumn('pm_score', lit(None))\n\n  # # update each row with new component id\n  max_comp = results_pc_pm.agg({\"component\": \"max\"}).collect()[0][0]\n\n  # # find total number of rows needed for new component\n  new_comps_needed = under_threshold.count()\n\n  # # create list of sequential components with lengths = length of under threshold\n  new_components = [x for x in range(max_comp + 1, max_comp + new_comps_needed + 1)]\n  # print(new_components)\n\n  # # update components\n  b = sqlContext.createDataFrame([(l,) for l in new_components], ['component'])\n\n  # #add 'sequential' index and join both dataframe to get the final result\n  under_threshold = under_threshold.withColumn(\"row_idx\",  row_number().over(Window.orderBy(monotonically_increasing_id())))\n  b = b.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n\n\n  # concat the datasets back together\n  under_threshold = under_threshold.join(b, under_threshold.row_idx == b.row_idx).drop(\"row_idx\")\n\n  # reorder columns for concatenation\n  under_threshold = under_threshold.select(over_threshold.columns)\n\n  # # Combine data back into 1 df\n  results_pc_pm = under_threshold.union(over_threshold)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Dissolve Mega Clusters","showTitle":true,"inputWidgets":{},"nuid":"0f8963f5-5d03-4aad-99e3-c8cbef345e12"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_AKA_order = results_pc_pm.orderBy(['pc_flag', 'dispatch_timestamp'], ascending=[0,0])\ndf_AKA_order = df_AKA_order.withColumn('first_name', upper(col('first_name'))).withColumn('last_name', upper(col('last_name')))\ndf_AKA_order.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{intDataDir}/delta/GOLD/df_AKA_order\")\ndf_AKA_order = spark.read.format(\"delta\").load(f\"{intDataDir}/delta/GOLD/df_AKA_order\")\n#aggregate first names, sort on component, count(most frequent name), average pm score\nfirst_name_AKA = df_AKA_order.groupBy('component','first_name')\\\n  .agg(count('first_name').alias('cnt'), first('pc_flag').alias('flag'), avg('pm_score').alias('score_avg'), first('dispatch_timestamp').alias('dis_time'))\\\n  .orderBy(['component','cnt','flag', 'score_avg', 'dis_time'], ascending=[1,0,0,0,0])\n\nwindow_spec = Window.partitionBy('component').orderBy(col('cnt').desc(),col('flag'), col('score_avg').desc(), col('dis_time'))\n\n#rank the names into top 5 and pivot into their own cols\nfirst_name_rank = first_name_AKA.withColumn('rank', rank().over(window_spec)).filter(col('rank') <= 5).groupBy('component').pivot('rank').agg(first('first_name'))\n\n#make sure there are at least 5 cols of AKAs\nfor i in range(1,6):\n  if str(i) in first_name_rank.columns:\n    print(str(i),'exists')\n  else:\n#     first_name_rank = first_name_rank.withColumn(str(i), lit(None).cast(NullType()))\n    first_name_rank = first_name_rank.withColumn(str(i), lit(None).cast(StringType()))\n\n#rename cols\nfirst_name_rank = first_name_rank.withColumnRenamed('1','aka_first_name_1')\\\n.withColumnRenamed('2','aka_first_name_2')\\\n.withColumnRenamed('3','aka_first_name_3')\\\n.withColumnRenamed('4','aka_first_name_4')\\\n.withColumnRenamed('5','aka_first_name_5')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"AKA Names","showTitle":true,"inputWidgets":{},"nuid":"65b01370-712b-4b9f-9ed9-a51bc431660e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#aggregate last names, sort on component, count(most frequent name), average pm score\n\nlast_name_AKA = df_AKA_order.groupBy('component','last_name')\\\n  .agg(count('last_name').alias('cnt'), first('pc_flag').alias('flag'), avg('pm_score').alias('score_avg'), first('dispatch_timestamp').alias('dis_time'))\\\n  .orderBy(['component','cnt','flag', 'score_avg', 'dis_time'], ascending=[1,0,0,0,0])\n\nwindow_spec = Window.partitionBy('component').orderBy(col('cnt').desc(),col('flag'), col('score_avg').desc(), col('dis_time'))\n\n#rank the names into top 5 and pivot into their own cols\nlast_name_rank = last_name_AKA.withColumn('rank', rank().over(window_spec)).filter(col('rank') <= 5).groupBy('component').pivot('rank').agg(first('last_name'))\n\n#make sure there are at least 5 cols of AKAs\nfor i in range(1,6):\n  if str(i) in last_name_rank.columns:\n    print(str(i),'exists')\n  else:\n    last_name_rank = last_name_rank.withColumn(str(i), lit(None).cast(StringType()))\n\n#rename cols    \nlast_name_rank = last_name_rank.withColumnRenamed('1','aka_last_name_1')\\\n.withColumnRenamed('2','aka_last_name_2')\\\n.withColumnRenamed('3','aka_last_name_3')\\\n.withColumnRenamed('4','aka_last_name_4')\\\n.withColumnRenamed('5','aka_last_name_5')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c85a368-6561-4feb-9ec4-1ad4a3c88d91"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#join the two tables to original\n\nresult_cols = ['component', 'patient_id', 'pcr_number', 'last_name', 'first_name', 'middle_name', 'address', 'city', 'county', 'state', 'zip_code', 'ssn', 'gender', 'race', 'age', 'age_units', 'dob', 'state_issuing_drivers_license', 'drivers_license_number', 'alternative_address', 'mbi', 'agency_unique_state_id', 'agency_id', 'agency_state', 'uuid', 'source', 'official_name_flag', 'pm_overwrite_flag', 'dispatch_timestamp', 'pcr_received_timestamp', 'claims_update_timestamp', 'action', 'mp_id', 'pc_flag', 'pm_score', 'er_flag']\nresults_pc_pm = results_pc_pm.select(result_cols).alias('a').join((first_name_rank.alias('b')), on=['component'], how='left').select('a.*','b.*')\n\nresults_pc_pm = results_pc_pm.alias('a').join((last_name_rank.alias('b')), on=['component'], how='left').select('a.*','b.*')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b51228c-a995-43de-a27e-6d00dc173e0c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(results_pc_pm\n .select('patient_id', 'pcr_number', 'last_name', 'first_name', 'middle_name', 'address', 'city', 'county', 'state', 'zip_code', 'ssn', 'gender', 'race', 'age', 'age_units', 'dob', 'state_issuing_drivers_license', 'drivers_license_number', 'alternative_address', 'mbi', 'agency_unique_state_id', 'agency_id', 'agency_state', 'uuid', 'source', 'official_name_flag', 'pm_overwrite_flag', 'dispatch_timestamp', 'pcr_received_timestamp', 'claims_update_timestamp', 'action', 'mp_id', 'pc_flag', 'pm_score', 'aka_first_name_1', 'aka_first_name_2',\n'aka_first_name_3','aka_first_name_4','aka_first_name_5','aka_last_name_1','aka_last_name_2','aka_last_name_3','aka_last_name_4','aka_last_name_5', 'er_flag')\n .createOrReplaceTempView(\"PCR_updated\")\n)\nspark.sql(\"INSERT OVERWRITE TABLE PCR_master SELECT * from PCR_updated\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Update PCR_master Table with Latest Data ","showTitle":true,"inputWidgets":{},"nuid":"420760bb-ec52-4aa0-883d-f678e5ba5bff"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql('truncate table weights')\nw_schema = StructType([\n  StructField('param_name', StringType(), False),\n  StructField('param_weight', FloatType(), False)\n])\nweights = [('Threshold_Percentage', float(match_threshold)), ('Last_Name_WP', float(w_LName)), ('First_Name_WP', float(w_FName)), ('Middle_Name_WP', float(w_MName)), \n          (\"Address_WP\", float(w_Street)), ('City_WP', float(w_City)), ('County_WP', float(w_County)), ('State_WP', float(w_State)), ('ZIP_Code_WP', float(w_Zip)), \n           ('SSN_WP', float(w_SSN)), ('Gender_WP', float(w_Gender)), ('Race_WP', float(w_Race)), ('Age_WP', float(w_Age)), ('DOB_WP', float(w_Dob)), ('Drivers_License_Number_WP', float(w_Lic)), ('MBI_WP', float(w_MBI)), ('State_Issuing_Drivers_License_WP', float(w_StateDL))]\nspark.createDataFrame(weights, w_schema).createOrReplaceTempView('weights_curr')\nspark.sql('''MERGE INTO weights a using weights_curr b  ON a.param_name==b.param_name\n  WHEN MATCHED \n    THEN UPDATE SET *\n  WHEN NOT MATCHED \n    THEN INSERT *''')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Build Output for Weights","showTitle":true,"inputWidgets":{},"nuid":"32902303-e27f-4a73-8fcd-2f6a2340470f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#updated_matched_results\ndf_matched = (spark.sql('select * from pcr_master where mp_id in (Select mp_id from pcr_master group by mp_id having count(*)>1)')\n        .withColumnRenamed('address', 'home_address')\n        .withColumnRenamed('city', 'home_city')\n        .withColumnRenamed('county', 'home_county')\n        .withColumnRenamed('state', 'home_state')\n        .withColumnRenamed('zip_code', 'home_zip_code')\n        .withColumnRenamed('dob', 'date_of_birth')\n        .withColumnRenamed('alternative_address', 'alternate_home_address')\n        .withColumn('pm_status', lit('M'))\n        .withColumnRenamed('mp_id', 'mpid')\n        .withColumnRenamed('pc_flag', 'pm_record_type')\n        .withColumn('pm_date', current_date())\n        .withColumn('pm_timestamp', current_timestamp())\n        .select('patient_id', 'pcr_number', 'last_name', 'first_name', 'middle_name', 'home_address', 'home_city', 'home_county', 'home_state', 'home_zip_code', 'ssn', 'gender', 'race', 'age', 'age_units', 'date_of_birth', 'state_issuing_drivers_license', 'drivers_license_number', 'alternate_home_address', 'mbi', 'agency_unique_state_id', 'agency_id', 'agency_state', 'uuid', 'source', 'official_name_flag', 'pm_overwrite_flag', 'dispatch_timestamp', 'pcr_received_timestamp', 'claims_update_timestamp', 'action', 'mpid', 'pm_score', 'pm_status', 'pm_record_type', 'pm_date', 'pm_timestamp','aka_first_name_1','aka_first_name_2','aka_first_name_3','aka_first_name_4','aka_first_name_5','aka_last_name_1','aka_last_name_2','aka_last_name_3','aka_last_name_4','aka_last_name_5'))\n\ndf_matched = df_matched.withColumn('pm_score', df_matched['pm_score'].cast(IntegerType()))\\\n                                        .withColumn('dispatch_timestamp', date_format('dispatch_timestamp', 'dd-MMM-yy hh.mm.ss.SSSSSSSSS a'))\\\n                                        .withColumn('pcr_received_timestamp', date_format('pcr_received_timestamp', 'dd-MMM-yy hh.mm.ss.SSSSSSSSS a'))\\\n                                        .withColumn('claims_update_timestamp', date_format('claims_update_timestamp', 'dd-MMM-yy hh.mm.ss.SSSSSSSSS a'))\\\n                                        .withColumn('pm_timestamp', date_format('pm_timestamp', 'dd-MMM-yy hh.mm.ss.SSSSSSSSS a'))\n\n# #updated_unmatched_results\ndf_unmatched = (spark.sql(\"Select * from pcr_master where mp_id not in (Select mp_id from pcr_master group by mp_id having count(*)>1)\")\n        .withColumnRenamed('address', 'home_address')\n        .withColumnRenamed('city', 'home_city')\n        .withColumnRenamed('county', 'home_county')\n        .withColumnRenamed('state', 'home_state')\n        .withColumnRenamed('zip_code', 'home_zip_code')\n        .withColumnRenamed('dob', 'date_of_birth')\n        .withColumnRenamed('alternative_address', 'alternate_home_address')\n        .withColumn('pm_status', lit('U'))\n        .withColumnRenamed('mp_id', 'mpid')\n        .withColumnRenamed('pc_flag', 'pm_record_type')\n        .withColumn('pm_date', current_date())\n        .withColumn('pm_timestamp', current_timestamp())\n        .select('patient_id', 'pcr_number', 'last_name', 'first_name', 'middle_name', 'home_address', 'home_city', 'home_county', 'home_state', 'home_zip_code', 'ssn', 'gender', 'race', 'age', 'age_units', 'date_of_birth', 'state_issuing_drivers_license', 'drivers_license_number', 'alternate_home_address', 'mbi', 'agency_unique_state_id', 'agency_id', 'agency_state', 'uuid', 'source', 'official_name_flag', 'pm_overwrite_flag', 'dispatch_timestamp', 'pcr_received_timestamp', 'claims_update_timestamp', 'action', 'mpid', 'pm_score', 'pm_status', 'pm_record_type', 'pm_date', 'pm_timestamp','aka_first_name_1','aka_first_name_2','aka_first_name_3','aka_first_name_4','aka_first_name_5','aka_last_name_1','aka_last_name_2','aka_last_name_3','aka_last_name_4','aka_last_name_5'))\n\n\ndf_unmatched = df_unmatched.withColumn('pm_score', df_unmatched['pm_score'].cast(IntegerType()))\\\n                                            .withColumn('dispatch_timestamp', date_format('dispatch_timestamp', 'dd-MMM-yy hh.mm.ss.SSSSSSSSS a'))\\\n                                            .withColumn('pcr_received_timestamp', date_format('pcr_received_timestamp', 'dd-MMM-yy hh.mm.ss.SSSSSSSSS a'))\\\n                                            .withColumn('claims_update_timestamp', date_format('claims_update_timestamp', 'dd-MMM-yy hh.mm.ss.SSSSSSSSS a'))\\\n                                            .withColumn('pm_timestamp', date_format('pm_timestamp', 'dd-MMM-yy hh.mm.ss.SSSSSSSSS a'))\\\n                                            .withColumn('pm_record_type', lit(None).cast(StringType())) #change 'P' to null for unmatched records"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create matched and unmatched dataframes","showTitle":true,"inputWidgets":{},"nuid":"1778a231-774c-4742-ac57-518ddc07566b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print('***MATCHED COUNTS***')\nprint('Matched count:',df_matched.count())\nprint('Matched distinct count:',df_matched.distinct().count())\nprint('Count MPIDs:',df_matched.select('mpid').count())\nprint('Unique MPIDs:',df_matched.select('mpid').distinct().count())\nprint('Unique Patient IDs:',df_matched.select('patient_id').distinct().count())\nprint('pm_status count:',df_matched.select('pm_status').count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7ef203e-e5a7-4ab2-b421-cde511b504bb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print('***UNMATCHED COUNTS***')\nprint('Unmatched count:',df_unmatched.count())\nprint('Unmatched distinct count:',df_unmatched.distinct().count())\nprint('Count MPIDs:',df_unmatched.select('mpid').count())\nprint('Unique MPIDs:',df_unmatched.select('mpid').distinct().count())\nprint('Unique Patient IDs:',df_unmatched.select('patient_id').distinct().count())\nprint('pm_status count:',df_unmatched.select('pm_status').count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1322fd5a-6c2b-4d4a-9aff-66b43a74f61d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["w_schema = StructType([\n  StructField('param_name', StringType(), False),\n  StructField('param_weight', FloatType(), False)\n])\nweights = [('Threshold_Percentage', float(match_threshold)), ('Last_Name_WP', float(w_LName)), ('First_Name_WP', float(w_FName)), ('Middle_Name_WP', float(w_MName)), \n          (\"Address_WP\", float(w_Street)), ('City_WP', float(w_City)), ('County_WP', float(w_County)), \n          ('State_WP', float(w_State)), ('ZIP_Code_WP', float(w_Zip)), ('SSN_WP', float(w_SSN)), \n          ('Gender_WP', float(w_Gender)), ('Race_WP', float(w_Race)), ('Age_WP', float(w_Age)),\n          ('DOB_WP', float(w_Dob)), ('Drivers_License_Number_WP', float(w_Lic)), ('MBI_WP', float(w_MBI)), ('State_Issuing_Drivers_License_WP', float(w_StateDL))]\ndf_weights = spark.createDataFrame(weights, w_schema)\ndf_weights_reformat = df_weights.withColumn('groupCol', lit('g1'))\ndf_weights_reformat = df_weights_reformat.groupBy('groupCol').pivot('param_name').mean('param_weight').drop('groupCol')\ndf_weights_reformat = df_weights_reformat.select('Threshold_Percentage','Last_Name_WP','First_Name_WP','Middle_Name_WP','Address_WP','City_WP', 'County_WP','State_WP','Zip_Code_WP','SSN_WP','Gender_WP','Race_WP','Age_WP','DOB_WP','State_Issuing_Drivers_License_WP',\n                                                'Drivers_License_Number_WP','MBI_WP')\ndbutils.fs.rm(f\"{tempDataDir}/weights\", True)\ndf_weights_reformat.coalesce(1).write.mode('overwrite').format('csv').option('header', 'true').option('delimiter', '|').option('emptyValue','').save(f\"{tempDataDir}/weights\")\n\n#updated_matched_results\ndbutils.fs.rm(f\"{tempDataDir}/updated_matched\", True)\ndf_matched.coalesce(1).write.mode('overwrite').format('csv').option('header', 'true').option('delimiter', '|').option('emptyValue','').save(f\"{tempDataDir}/updated_matched\")\n\n# #updated_unmatched_results\ndbutils.fs.rm(f\"{tempDataDir}/updated_unmatched\", True)\ndf_unmatched.coalesce(1).write.mode('overwrite').format('csv').option('header', 'false').option('delimiter', '|').option('emptyValue','').save(f\"{tempDataDir}/updated_unmatched\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write PM Output a Pipe-delimited CSV File (customer requests all outputs to be placed in a single file)","showTitle":true,"inputWidgets":{},"nuid":"b3520fc5-adb8-4afb-ab13-45e70778a936"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.notebook.exit(\"Step 3: PCR Batch Dedup with Sample Data completed successfully\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a124d805-b8a8-4f00-8153-8dd924c28fd4"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"11_PCR_Batch_PM_Processing","dashboards":[],"notebookMetadata":{},"language":"python","widgets":{},"notebookOrigID":2261646131270709}},"nbformat":4,"nbformat_minor":0}
