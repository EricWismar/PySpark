{"cells":[{"cell_type":"code","source":["%run ./00_functions_and_libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import required libraries and define functions","showTitle":true,"inputWidgets":{},"nuid":"69dc16e7-b4fd-4789-848d-6d3199195528"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./01_params"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Creates dict containing required variables, filepaths etc.","showTitle":true,"inputWidgets":{},"nuid":"dcbbb1ae-36a3-4c27-b6a7-1dd890de6df6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.conf.set('spark.sql.execution.arrow.enabled', True)\nspark.conf.set('spark.sql.execution.arrow.fallback.enabled', False)\nspark.conf.set(\"spark.sql.session.timeZone\", \"America/New_York\")\nspark.conf.set(\n  params[\"AzureSASLocation\"],\n  dbutils.secrets.get(scope=params[\"AzureSASScope\"],key=params[\"AzureSASKey\"])\n)\nspark.sql('use {}'.format(params[\"Database\"]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Environment Config","showTitle":true,"inputWidgets":{},"nuid":"7b2fae90-aa64-4498-9c65-fbe434f8c25b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["inbound_file=dbutils.fs.ls(f'{params[\"AzureET3Container\"]}/prod/inbound/')[2][0]\ninbound_filename=dbutils.fs.ls(f'{params[\"AzureET3Container\"]}/prod/inbound/')[2][1]\nprint(\"inbound filepath-> {}\".format(inbound_file),\"\\n\",\"inbound filename -> {}\".format(inbound_filename))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Get the inbound file","showTitle":true,"inputWidgets":{},"nuid":"65d04739-0819-4793-87b3-0e05665aa364"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sample_data = spark.read.format('csv').option('header', True).option('delimiter', '|').load(inbound_file)\nassert sample_data.schema == InboundFileExpectedSchema, \"Inbound File Does Not Match Expected Schema!\"\nprint(\"Inbound File Matches Expected Schema\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Get the schema of the inbound data","showTitle":true,"inputWidgets":{},"nuid":"a442adf5-00e1-41a0-b1f0-4d1cfc800507"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(sample_data.groupBy('agency_id').count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Check if data in \"patient_id\" are unique","showTitle":true,"inputWidgets":{},"nuid":"cfa639c0-969a-4fb2-b354-e3565b7faf5b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(sample_data.count())\nsample_data.filter('mbi is not null and length(mbi)>0').count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c729c587-efa5-47c8-b73b-541de6e2ecd4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sample_data_dedup = sample_data.dropDuplicates(subset=['patient_id'])\nsample_data_dedup.groupBy('patient_id').count().filter('count>1').display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"If there are records with duplicated \"patient_id\", keep only one record.","showTitle":true,"inputWidgets":{},"nuid":"514bb61d-d8f7-4563-8a3d-a5067b5685cc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sample_data_clean = (sample_data_dedup\n  .withColumnRenamed('home_address', 'address')\n  .withColumnRenamed('home_city', 'city')\n  .withColumnRenamed('home_county', 'county')\n  .withColumnRenamed('home_state', 'state')\n  .withColumnRenamed('home_zip_code', 'zip_code')\n  .withColumnRenamed('date_of_birth', 'dob')\n  .withColumnRenamed('alternate_home_address', 'alternative_address'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Rename columns to match the PM model schema","showTitle":true,"inputWidgets":{},"nuid":"a7fc1bd1-7346-4db6-b1f3-6c1b5dabbe0e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Here we focus on records that have been overridden by a user.\n# We need to store these now so we can enforce them in the next notebook.\n# We will need to lookup the current groupings for each identified record in the PCR_master table.\n# To make sure we are looking at the correct version, this step should occur before updating the PCR_master table\n\n# Retrieve the latest groupings. We only need record IDs and grouping IDs here.\npcr_master_current = spark.sql(\"Select patient_id, mp_id from pcr_master\")\n\n# From our new dataset, keep the records where the overwrite flag is \"Y\"\nnew_override_ids = (sample_data_clean\n                    .filter(upper(col(\"PM_OVERWRITE_FLAG\"))==\"Y\")\n                    .select(\"PATIENT_ID\")\n                    .withColumnRenamed(\"PATIENT_ID\", \"flagged_id\")\n                    .distinct()\n                   )\n\n# Join with the pcr_master table on record IDs to get the mp_id associated with each user override\nnew_override_ids_with_mpids = (new_override_ids\n                               .join(pcr_master_current, new_override_ids.flagged_id==pcr_master_current.patient_id)\n                               .drop(\"patient_id\")\n                              )\n\n# Join with the pcr_master table again, this time on mp_id, to get all other records that will form a \"delinked pair\"\n# Also remove, \"self-pairs\" where the id is the same on both sides of the pair.\nnew_override_pairs = (new_override_ids_with_mpids\n                      .join(pcr_master_current, \"mp_id\", \"left\")\n                      .withColumnRenamed(\"patient_id\", \"delinked_partner_id\")\n                      .filter(col(\"flagged_id\")!=col(\"delinked_partner_id\"))\n                      .select(\"flagged_id\", \"delinked_partner_id\")\n                     )\nnew_override_pairs.createOrReplaceTempView(\"delinking_changes\")\n\n# Merge new delinked records into delinked_pairs\nspark.sql(\"\"\"\n  MERGE INTO delinked_pairs M USING delinking_changes C\n  ON M.flagged_id == C.flagged_id AND M.delinked_partner_id == C.delinked_partner_id\n  WHEN MATCHED \n    THEN UPDATE SET *\n  WHEN NOT MATCHED \n    THEN INSERT *\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Update delinked_pairs with new overrides","showTitle":true,"inputWidgets":{},"nuid":"22b2bb23-57f8-4fe8-8151-bc3970bfbd49"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["batch_df = sample_data_clean\n\n#######################################################################\n# Convert Timestamp fields from string to timestamp\n#######################################################################\nbatch_df = (batch_df.withColumn(\"DISPATCH_TIMESTAMP\", to_timestamp(\"DISPATCH_TIMESTAMP\", \"dd-MMM-yy hh.mm.ss.SSSSSSSSS a\"))\\\n                    .withColumn(\"PCR_RECEIVED_TIMESTAMP\", to_timestamp(\"PCR_RECEIVED_TIMESTAMP\", \"dd-MMM-yy hh.mm.ss.SSSSSSSSS a\"))\\\n                    .withColumn(\"CLAIMS_UPDATE_TIMESTAMP\", to_timestamp(\"CLAIMS_UPDATE_TIMESTAMP\", \"dd-MMM-yy hh.mm.ss.SSSSSSSSS a\")))\n\n################################################################\n# Add name alias columns (aka columns)  to batch data\n################################################################\nbatch_df = (batch_df.withColumn('aka_first_name_1', lit(None).cast(StringType()))\n                   .withColumn('aka_first_name_2', lit(None).cast(StringType()))\n                   .withColumn('aka_first_name_3', lit(None).cast(StringType()))\n                   .withColumn('aka_first_name_4', lit(None).cast(StringType()))\n                   .withColumn('aka_first_name_5', lit(None).cast(StringType()))\n                   .withColumn('aka_last_name_1', lit(None).cast(StringType()))\n                   .withColumn('aka_last_name_2', lit(None).cast(StringType()))\n                   .withColumn('aka_last_name_3', lit(None).cast(StringType()))\n                   .withColumn('aka_last_name_4', lit(None).cast(StringType()))\n                   .withColumn('aka_last_name_5', lit(None).cast(StringType())))\n                   \n#####################################\n#      DATA CLEANSING\n#####################################\n\n\nbatch_df = batch_df.withColumn('address', when(col('address') == ' ', None).otherwise(col('address')))\nbatch_df = batch_df.withColumn('address', when(trim(lower(col('address'))).isin(['homeless','refused','unknown','na','transient','homless','no fixed address',\n                                                                                 '2400 cypress st','2401 cypress st','2400 cypress','2400 cypress st.','2400 cypress st presbyterian night shelter',\n                                                                                 '2400 cypress st homeless','2401 cypress','2400 cypress ave','2400 cypress homeless','2400 cyprus','e presidio st / cypress st homeless',\n                                                                                 'cypress st / e lancaster ave','1513 e persideo','2401 cypress st women and childrenï¿½s shelter',\n                                                                                 '620 fallsway','620 the fallsway','421 fallsway','725 fallsway','620 fallsway homeless',\n                                                                                 '1513 e presidio st','1513 e presidio','1513 presidio','1513 presidio st','1513 e presidio junction','1513 e. presidio','1513 e precidio',\n                                                                                '600 n henderson st','1321 e lancaster ave','1331 e lancaster ave','2700 n charles st', '123 homeless','1234 homeless','9999 homeless',\n                                                                                'unable to obtain', '1306 goodwood ave','2605 loyola southway','2434 w belvedere av']), None).otherwise(col('address')))\nbatch_df = batch_df.withColumn('address', regexp_replace('address', '\\?', ' '))\n\nbatch_df = batch_df.withColumn('ssn', when(col('ssn').isin(['999999999','000000000','222222222','777777777','555555555','111111111','666666666','999999990','998999999','123456789']), None).otherwise(col('ssn')))\n# https://www.ssa.gov/employer/randomization.html add SSNs combos that aren't possible.\n\nbatch_df = batch_df.withColumn('first_name_temp', when((trim(lower(col('first_name'))) == 'john') & (trim(lower(col('last_name'))) == 'doe'), None).otherwise(col('first_name')))\nbatch_df = batch_df.withColumn('last_name', when((trim(lower(col('first_name'))) == 'john') & (trim(lower(col('last_name'))) == 'doe'), None).otherwise(col('last_name')))\nbatch_df = batch_df.withColumn('first_name', col('first_name_temp')).drop('first_name_temp')\n                               \nbatch_df = batch_df.withColumn('first_name_temp', when((trim(lower(col('first_name'))) == 'jane') & (trim(lower(col('last_name'))) == 'doe'), None).otherwise(col('first_name')))\nbatch_df = batch_df.withColumn('last_name', when((trim(lower(col('first_name'))) == 'jane') & (trim(lower(col('last_name'))) == 'doe'), None).otherwise(col('last_name')))\nbatch_df = batch_df.withColumn('first_name', col('first_name_temp')).drop('first_name_temp')\n                               \nbatch_df = batch_df.withColumn('first_name_temp', when((trim(lower(col('first_name'))) == 'unknown') & (trim(lower(col('last_name'))) == 'unknown'), None).otherwise(col('first_name')))\nbatch_df = batch_df.withColumn('last_name', when((trim(lower(col('first_name'))) == 'unknown') & (trim(lower(col('last_name'))) == 'unknown'), None).otherwise(col('last_name')))\nbatch_df = batch_df.withColumn('first_name', col('first_name_temp')).drop('first_name_temp')\n\n#Convert any non-numeric values to Null, by using the string library's ascii_uppercase variable containing all letters of the alphabet. We don't want\n#to maintain a listing of any or all arbitrary non-numeric values in the age column, rather we'll split apart the column into an array and use an array_overlap to see \n#if any of the items are in an array of the alphabet\nbatch_df=batch_df.withColumn('ageArray',array_except(split(upper(col('AGE')),''),array(lit(\"\"))))\\\n.withColumn('ascii',array_except(split(lit(string.ascii_uppercase),''),array(lit(\"\"))))\\\n.withColumn('NonNumeric',when(arrays_overlap(col('ageArray'),('ascii')), 'Y').otherwise('N'))\\\n.withColumn('AGE',when(col('NonNumeric')=='Y',None).otherwise(batch_df.AGE))\\\n.drop('ageArray','ascii','NonNumeric')\n\n#####################################\n#     END OF CLEANSING\n#####################################\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Ingest next PCR batch. Add alias Columns (aka columns) to batch data","showTitle":true,"inputWidgets":{},"nuid":"7eaf517f-602a-4c89-ac86-be44af309e83"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#################################################################\n#   Identify data worth passing to ER model\n#################################################################\n\n# er flag is true if nonnull count > 4.\n\nbatch_df = batch_df.withColumn(\"nonnull_count\", nonnull_count(col(\"FIRST_NAME\"), col(\"LAST_NAME\"), col(\"MIDDLE_NAME\"), col(\"address\"),\n                                                                    col('city'), col('state'), col('zip_code'), col('SSN'), col('county'),\n                                                                    col('GENDER'), col('RACE'), col('dob'), col('MBI'), col('alternative_address'),\n                                                                    col('AGE'), col('AGE_UNITS'), col('DRIVERS_LICENSE_NUMBER'), col('STATE_ISSUING_DRIVERS_LICENSE')\n                                                                   ))\n\nbatch_df = batch_df.withColumn('er_flag', when(col('nonnull_count') > 10, lit(1)).otherwise(lit(0))).drop('nonnull_count')\n\n  \nmpid_df = spark.sql(\"select * from mp_id\")                                 # Read mp_id data into dataframe\n\nmpid_df = mpid_df.withColumn('mp_id', regexp_replace('mp_id','M','').cast(IntegerType())) #remove leading 'M' and cast to Integer\n\njoin_result = (batch_df                                                    # Left Join batch data with mp_id on recordId\n               .join(mpid_df, \"patient_id\", \"left\")                            # (to assign original mp_id to existing records)\n#                .withColumn(\"mp_id\", coalesce(col(\"mp_id\"), col(\"pcr_id\")))\n               .withColumn(\"mp_id\", coalesce(col(\"mp_id\"), col(\"patient_id\")))\n               .withColumn(\"pc_flag\", lit(\"P\"))\n               .withColumn(\"pm_score\", lit(None))\n              )\n\njoin_result.createOrReplaceTempView(\"PCR_master_changes\")\n\n# mpid_merge_source = join_result.select(\"pcr_id\", \"mp_id\")                  # Select recId:mp_id from join_result to update mp_id\nmpid_merge_source = join_result.select(\"patient_id\", \"mp_id\")                  # Select recId:mp_id from join_result to update mp_id\nmpid_merge_source.createOrReplaceTempView(\"mpid_changes\")\n\n# # Merge new mp_ids into mp_id table\n# spark.sql(\"\"\"\n#   MERGE INTO mp_id M USING mpid_changes C\n#   ON M.pcr_id == C.pcr_id\n#   WHEN MATCHED THEN UPDATE SET *\n#   WHEN NOT MATCHED THEN INSERT *\n# \"\"\")\n\n# # Merge join_result into Master table\n# spark.sql(\"\"\"\n#   MERGE INTO PCR_master M USING PCR_master_changes C\n#   ON M.pcr_id == C.pcr_id\n#   WHEN MATCHED\n#     THEN UPDATE SET *\n#   WHEN NOT MATCHED and C.action == \"ADD\"\n#     THEN INSERT *\n# \"\"\")\n\n# Merge new mp_ids into mp_id table\nspark.sql(\"\"\"\n  MERGE INTO mp_id M USING mpid_changes C\n  ON M.patient_id == C.patient_id\n  WHEN MATCHED \n    THEN UPDATE SET *\n  WHEN NOT MATCHED \n    THEN INSERT *\n\"\"\")\n\n# Merge join_result into Master table\n# <<TO DO>>: Discuss with NewWave: should we rely on key or action provided in the data\n# spark.sql('''\n#   MERGE INTO PCR_master M USING PCR_master_changes C\n#   ON M.patient_id == C.patient_id\n#   WHEN MATCHED and C.action == \"U\" \n#     THEN UPDATE SET *\n#   WHEN NOT MATCHED and C.action == \"A\"\n#     THEN INSERT *\n#  ''')\n\n## Take out the \"action\" flag. We will rely on patient_id solely to determine if we have processed the record or not\nspark.sql('''\n  MERGE INTO pcr_master M USING PCR_master_changes C\n  ON M.patient_id == C.patient_id\n  WHEN MATCHED \n    THEN UPDATE SET *\n  WHEN NOT MATCHED\n    THEN INSERT *\n ''')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3a63c70-7a29-4704-939c-ad612375e288"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect count(*) from mp_id"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Record count in mp_id table","showTitle":true,"inputWidgets":{},"nuid":"bb12aff6-9aea-4f46-8b63-c411a8d4d50f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect count(*) from PCR_master"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Record count in PCR_master table","showTitle":true,"inputWidgets":{},"nuid":"7018923e-61e0-4f5b-80cb-92ecdac60b57"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql('select * from pcr_master@v61').count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f6ebcc9-67d7-4e93-91c2-40e46167a87d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["match_cols=[\"FIRST_NAME\",\"LAST_NAME\",\"MIDDLE_NAME\",\"address\",'city','state','zip_code','SSN','county','GENDER','RACE','dob','MBI','AGE','DRIVERS_LICENSE_NUMBER','STATE_ISSUING_DRIVERS_LICENSE']\n\nResults=[]\ndf=spark.sql('select * from pcr_master')\n'''df=spark.sql('select * from pcr_master@v61')\ndf=df.withColumn(\"nonnull_count\", nonnull_count(col(\"FIRST_NAME\"), col(\"LAST_NAME\"), col(\"MIDDLE_NAME\"), col(\"address\"),\n                                                                    col('city'), col('state'), col('zip_code'), col('SSN'), col('county'),\n                                                                    col('GENDER'), col('RACE'), col('dob'), col('MBI'), col('alternative_address'),\n                                                                    col('AGE'), col('AGE_UNITS'), col('DRIVERS_LICENSE_NUMBER'), col('STATE_ISSUING_DRIVERS_LICENSE'))).withColumn('er_flag', when(col('nonnull_count') > 10, lit(1)).otherwise(lit(0))).drop('nonnull_count')'''\ndf = df.filter('er_flag==1')\ncount=df.count()\nfor i in match_cols:\n  a = i\n  b = df.filter('{} is not null'.format(i)).count()\n  c = b/count\n  Results.append([a,b,c])\n  \nResults_df=pd.DataFrame(Results,columns=['Variable','Count_NotNull','Pct_NotNull'])\nprint(count)\nResults_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Predictor Variable Completeness (for qualifying PCRs)","showTitle":true,"inputWidgets":{},"nuid":"86ca163a-f366-4b40-9e94-51c60d7155d6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["archive_filepath=params[\"AzureET3Mount\"]+\"/prod/archive/inbound/\"+inbound_filename\ndbutils.fs.cp(inbound_file,\n              archive_filepath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Move inbound file to Archive","showTitle":true,"inputWidgets":{},"nuid":"91e490af-953b-4267-a267-405595a627da"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.notebook.exit(\"Step 2: PCR Batch Ingest with Sample Data completed successfully\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8c2a201-b6c9-4919-8c77-4abfb41bf47f"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"10_PCR_Batch_Ingest_Original","dashboards":[],"notebookMetadata":{},"language":"python","widgets":{},"notebookOrigID":3446138362803059}},"nbformat":4,"nbformat_minor":0}
